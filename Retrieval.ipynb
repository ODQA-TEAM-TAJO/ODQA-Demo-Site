{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93610a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "es_server = Popen(['/home/dr_lunars/elasticsearch-7.0.0/bin/elasticsearch'],stdout=PIPE, stderr=STDOUT)\n",
    "!sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/dr_lunars/elasticsearch-7.0.0/bin/elasticsearch-plugin install analysis-nori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77641aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/dr_lunars/elasticsearch-7.0.0/bin/elasticsearch-plugin install https://github.com/javacafe-project/elasticsearch-plugin/releases/download/v7.0.0/javacafe-analyzer-7.0.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539014b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_server.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc05f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "es_server = Popen(['/home/dr_lunars/elasticsearch-7.0.0/bin/elasticsearch'],stdout=PIPE, stderr=STDOUT)\n",
    "!sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\", timeout=300, max_retries=10, retry_on_timeout=True)\n",
    "\n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f657e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.create(index = 'document',\n",
    "                  body = {\n",
    "                      'settings':{\n",
    "                          'analysis':{\n",
    "                              'analyzer':{\n",
    "                                  'my_analyzer':{\n",
    "                                      \"type\": \"custom\",\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'decompound_mode':'mixed',\n",
    "                                      'stopwords':'_korean_',\n",
    "                                      'synonyms':'_korean_',\n",
    "                                      \"filter\": [\"lowercase\",\n",
    "                                                 \"my_shingle_f\",\n",
    "                                                 \"nori_readingform\",\n",
    "                                                 \"cjk_bigram\",\n",
    "                                                 \"decimal_digit\",\n",
    "                                                 \"stemmer\",\n",
    "                                                 \"trim\"]\n",
    "                                  },\n",
    "                                  'kor2eng_analyzer':{\n",
    "                                      'type':'custom',\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'filter': [\n",
    "                                          'trim',\n",
    "                                          'lowercase',\n",
    "                                          'javacafe_kor2eng'\n",
    "                                      ]\n",
    "                                  },\n",
    "                                  'eng2kor_analyzer': {\n",
    "                                      'type': 'custom',\n",
    "                                      'tokenizer': 'nori_tokenizer',\n",
    "                                      'filter': [\n",
    "                                          'trim',\n",
    "                                          'lowercase',\n",
    "                                          'javacafe_eng2kor'\n",
    "                                      ]\n",
    "                                  },\n",
    "                              },\n",
    "                              'filter':{\n",
    "                                  'my_shingle_f':{\n",
    "                                      \"type\": \"shingle\"\n",
    "                                  }\n",
    "                              }\n",
    "                          },\n",
    "                          'similarity':{\n",
    "                              'my_similarity':{\n",
    "                                  'type':'BM25',\n",
    "                              }\n",
    "                          }\n",
    "                      },\n",
    "                      'mappings':{\n",
    "                          'properties':{\n",
    "                              'title':{\n",
    "                                  'type':'keyword',\n",
    "                                  'copy_to':['title_kor2eng','title_eng2kor']\n",
    "                              },\n",
    "                              'title_kor2eng': {\n",
    "                                  'type': 'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'search_analyzer': 'kor2eng_analyzer'\n",
    "                              },\n",
    "                              'title_eng2kor': {\n",
    "                                  'type': 'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'search_analyzer': 'eng2kor_analyzer'\n",
    "                              },\n",
    "                              'text':{\n",
    "                                  'type':'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'similarity':'my_similarity',\n",
    "                              },\n",
    "                          }\n",
    "                      }\n",
    "                  }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c87bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "wiki_path = '../processed_wiki.pickle'\n",
    "with open(wiki_path, 'rb') as fr:\n",
    "    processed_wiki = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8cdc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "texts = []\n",
    "for p_data in processed_wiki:\n",
    "    for data in p_data:\n",
    "        titles.append(data['title'])\n",
    "        texts.append(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00223fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = {\n",
    "    'title': titles,\n",
    "    'text': texts\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c076428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ee75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from tqdm import tqdm\n",
    "\n",
    "buffer = []\n",
    "rows = 0\n",
    "\n",
    "for num in tqdm(range(len(df))):\n",
    "    article = {\"_id\": num,\n",
    "               \"_index\": \"document\", \n",
    "               \"title\" : df['title'][num],\n",
    "               \"text\" : df['text'][num]}\n",
    "\n",
    "    buffer.append(article)\n",
    "\n",
    "    rows += 1\n",
    "\n",
    "    if rows % 3000 == 0:\n",
    "        helpers.bulk(es, buffer)\n",
    "        buffer = []\n",
    "        print(\"Inserted {} articles\".format(rows), end=\"\\r\")\n",
    "\n",
    "if buffer:\n",
    "    helpers.bulk(es, buffer)\n",
    "\n",
    "print(\"Total articles inserted: {}\".format(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.create(index = 'qa',\n",
    "                  body = {\n",
    "                      'settings':{\n",
    "                          'analysis':{\n",
    "                              'analyzer':{\n",
    "                                  'my_analyzer':{\n",
    "                                      \"type\": \"custom\",\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'decompound_mode':'mixed',\n",
    "                                      'stopwords':'_korean_',\n",
    "                                      'synonyms':'_korean_',\n",
    "                                      \"filter\": [\"lowercase\",\n",
    "                                                 \"my_shingle_f\",\n",
    "                                                 \"nori_readingform\",\n",
    "                                                 \"cjk_bigram\",\n",
    "                                                 \"decimal_digit\",\n",
    "                                                 \"stemmer\",\n",
    "                                                 \"trim\"]\n",
    "                                  },\n",
    "                                  'kor2eng_analyzer':{\n",
    "                                      'type':'custom',\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'filter': [\n",
    "                                          'trim',\n",
    "                                          'lowercase',\n",
    "                                          'javacafe_kor2eng'\n",
    "                                      ]\n",
    "                                  },\n",
    "                                  'eng2kor_analyzer': {\n",
    "                                      'type': 'custom',\n",
    "                                      'tokenizer': 'nori_tokenizer',\n",
    "                                      'filter': [\n",
    "                                          'trim',\n",
    "                                          'lowercase',\n",
    "                                          'javacafe_eng2kor'\n",
    "                                      ]\n",
    "                                  },\n",
    "                              },\n",
    "                              'filter':{\n",
    "                                  'my_shingle_f':{\n",
    "                                      \"type\": \"shingle\"\n",
    "                                  }\n",
    "                              }\n",
    "                          },\n",
    "                          'similarity':{\n",
    "                              'my_similarity':{\n",
    "                                  'type':'BM25',\n",
    "                              }\n",
    "                          }\n",
    "                      },\n",
    "                      'mappings':{\n",
    "                          'properties':{\n",
    "                              'question':{\n",
    "                                  'type':'text',\n",
    "                                  'copy_to':['question_kor2eng','question_eng2kor']\n",
    "                              },\n",
    "                              'question_kor2eng': {\n",
    "                                  'type': 'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'search_analyzer': 'kor2eng_analyzer'\n",
    "                              },\n",
    "                              'question_eng2kor': {\n",
    "                                  'type': 'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'search_analyzer': 'eng2kor_analyzer'\n",
    "                              },\n",
    "                              'answer':{\n",
    "                                  'type':'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'similarity':'my_similarity',\n",
    "                              }\n",
    "                          }\n",
    "                      }\n",
    "                  }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb87db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(es.indices.get('qa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_v1 = load_dataset('squad_kor_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f667e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../ko_wiki_v1_squad.json\", \"r\") as f:\n",
    "    ko_wiki_v1_squad = json.load(f)\n",
    "\n",
    "with open(\"../ko_nia_normal_squad_all.json\", \"r\") as f:\n",
    "    ko_nia_normal_squad_all = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa54d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = dataset_v1['train']['question'] + dataset_v1['validation']['question'] + [i['paragraphs'][0]['qas'][0]['question'] for i in ko_wiki_v1_squad['data']] + [j['question'] for i in ko_nia_normal_squad_all['data'] for j in i['paragraphs'][0]['qas']]\n",
    "answers = [i['text'][0] for i in dataset_v1['train']['answers']] + [i['text'][0] for i in dataset_v1['validation']['answers']] + [i['paragraphs'][0]['qas'][0]['answers'][0]['text'] for i in ko_wiki_v1_squad['data']] + [j['answers'][0]['text'] for i in ko_nia_normal_squad_all['data'] for j in i['paragraphs'][0]['qas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'question':questions,'answer':answers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "buffer = []\n",
    "rows = 0\n",
    "\n",
    "for num in tqdm(range(len(df))):\n",
    "    article = {\"_id\": num,\n",
    "               \"_index\": \"qa\", \n",
    "               \"question\" : df['question'][num],\n",
    "               \"answer\" : df['answer'][num]}\n",
    "\n",
    "    buffer.append(article)\n",
    "\n",
    "    rows += 1\n",
    "\n",
    "    if rows % 3000 == 0:\n",
    "        helpers.bulk(es, buffer)\n",
    "        buffer = []\n",
    "        print(\"Inserted {} articles\".format(rows), end=\"\\r\")\n",
    "\n",
    "if buffer:\n",
    "    helpers.bulk(es, buffer)\n",
    "\n",
    "print(\"Total articles inserted: {}\".format(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2611265",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.delete('chatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b776c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.create(index = 'chatter',\n",
    "                  body = {\n",
    "                      'settings':{\n",
    "                          'analysis':{\n",
    "                              'analyzer':{\n",
    "                                  'my_analyzer':{\n",
    "                                      \"type\": \"custom\",\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'decompound_mode':'mixed',\n",
    "                                      'stopwords':'_korean_',\n",
    "                                      'synonyms':'_korean_',\n",
    "                                      \"filter\": [\"lowercase\",\n",
    "                                                 \"my_shingle_f\",\n",
    "                                                 \"nori_readingform\",\n",
    "                                                 \"cjk_bigram\",\n",
    "                                                 \"decimal_digit\",\n",
    "                                                 \"stemmer\",\n",
    "                                                 \"trim\"]\n",
    "                                  },\n",
    "                                  'kor2eng_analyzer':{\n",
    "                                      'type':'custom',\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'filter': [\n",
    "                                          'trim',\n",
    "                                          'lowercase',\n",
    "                                          'javacafe_kor2eng'\n",
    "                                      ]\n",
    "                                  },\n",
    "                                  'eng2kor_analyzer': {\n",
    "                                      'type': 'custom',\n",
    "                                      'tokenizer': 'nori_tokenizer',\n",
    "                                      'filter': [\n",
    "                                          'trim',\n",
    "                                          'lowercase',\n",
    "                                          'javacafe_eng2kor'\n",
    "                                      ]\n",
    "                                  },\n",
    "                              },\n",
    "                              'filter':{\n",
    "                                  'my_shingle_f':{\n",
    "                                      \"type\": \"shingle\"\n",
    "                                  }\n",
    "                              }\n",
    "                          },\n",
    "                          'similarity':{\n",
    "                              'my_similarity':{\n",
    "                                  'type':'BM25',\n",
    "                              }\n",
    "                          }\n",
    "                      },\n",
    "                      'mappings':{\n",
    "                          'properties':{\n",
    "                              'question':{\n",
    "                                  'type':'text',\n",
    "                                  'copy_to':['question_kor2eng','question_eng2kor']\n",
    "                              },\n",
    "                              'question_kor2eng': {\n",
    "                                  'type': 'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'search_analyzer': 'kor2eng_analyzer'\n",
    "                              },\n",
    "                              'question_eng2kor': {\n",
    "                                  'type': 'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'search_analyzer': 'eng2kor_analyzer'\n",
    "                              },\n",
    "                              'answer':{\n",
    "                                  'type':'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'similarity':'my_similarity',\n",
    "                              }\n",
    "                          }\n",
    "                      }\n",
    "                  }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aed509",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(es.indices.get('chatter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = [['AI가 뭐지?', '인공지능은 공학 및 과학을 통해 생각하는 기계를 만든 것 입니다.'],\n",
    "      ['AI가 뭐지?', 'AI는 인간 정신의 기능을 복제하는 하드웨어와 소프트웨어를 만드는 것과 관련된 과학 분야입니다.'],\n",
    "      ['마음이 있니?', '아마도 있지 않을까요?'],\n",
    "      ['마음이 있니?', '아마두요?'],\n",
    "      ['너는 마음이 있니?', \"사전적 '마음'의 의미에서는 조금 있어요.\"],\n",
    "      ['너는 어떤 언어로 만들어졌니?', '파이썬이죠!'],\n",
    "      ['너는 어떤 언어로 만들어졌니?', '저는 파이썬으로 만들어졌습니다.'],\n",
    "      ['너도 웃니?', '끼요요요오옷!'],\n",
    "      ['ㅋㅋㅋㅋㅋ','ㅋㅋㅋㅋㅋ'],\n",
    "      ['ㅋㅋㅋㅋㅋㅋ','ㅋㅋㅋㅋㅋㅋ'],\n",
    "      ['ㅋㅋㅋㅋㅋㅋㅋ','ㅋㅋㅋㅋㅋㅋㅋ'],\n",
    "      ['ㅋㅋㅋㅋㅋㅋㅋㅋ','ㅋㅋㅋㅋㅋㅋㅋㅋ'],\n",
    "      ['ㅋㅋㅋㅋㅋㅋㅋㅋㅋ','ㅋㅋㅋㅋㅋㅋㅋㅋㅋ'],\n",
    "      ['ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ','ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ'],\n",
    "      ['ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ','ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ'],\n",
    "      ['ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ','ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ'],\n",
    "      ['ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ','ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ'],\n",
    "      ['ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ','ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ'],\n",
    "      ['ㅋㅋㅋㅋ','ㅋㅋㅋㅋ'],\n",
    "      ['ㅋㅋㅋ','ㅋㅋㅋ'],\n",
    "      ['ㅋㅋ','ㅋㅋ'],\n",
    "      ['ㅋㅋㅋㅋ','하.하.하.하.'],\n",
    "      ['ㅋㅋㅋ','하.하.하.'],\n",
    "      ['ㅋㅋ','하.하.'],\n",
    "      ['ㅎㅎㅎㅎ','ㅎㅎㅎㅎ'],\n",
    "      ['ㅎㅎㅎ','ㅎㅎㅎ'],\n",
    "      ['ㅎㅎ','ㅎㅎ'],\n",
    "      ['ㅎㅎㅎㅎ','하.하.하.하.'],\n",
    "      ['ㅎㅎㅎ','하.하.하.'],\n",
    "      ['ㅎㅎ','하.하.'],\n",
    "      ['ㅠ','ㅠ'],\n",
    "      ['ㅠㅠ','ㅠㅠ'],\n",
    "      ['ㅠㅠㅠ','ㅠㅠㅠ'],\n",
    "      ['ㅠㅠㅠㅠ','ㅠㅠㅠㅠ'],\n",
    "      ['ㅠㅠㅠㅠㅠ','ㅠㅠㅠㅠㅠ'],\n",
    "      ['ㅠㅠㅠㅠㅠㅠ','ㅠㅠㅠㅠㅠㅠ'],\n",
    "      ['ㅠㅠㅠㅠㅠㅠㅠ','ㅠㅠㅠㅠㅠㅠㅠ'],\n",
    "      ['ㅠㅠㅠㅠㅠㅠㅠㅠ','ㅠㅠㅠㅠㅠㅠㅠㅠ'],\n",
    "      ['ㅠㅠㅠㅠㅠㅠㅠㅠㅠ','ㅠㅠㅠㅠㅠㅠㅠㅠㅠ'],\n",
    "      ['ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ','ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ'],\n",
    "      ['ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ','ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ'],\n",
    "      ['ㅜ'*1,'ㅜ'*1],\n",
    "      ['ㅜ'*2,'ㅜ'*2],\n",
    "      ['ㅜ'*3,'ㅜ'*3],\n",
    "      ['ㅜ'*4,'ㅜ'*4],\n",
    "      ['ㅜ'*5,'ㅜ'*5],\n",
    "      ['ㅜ'*6,'ㅜ'*6],\n",
    "      ['ㅜ'*7,'ㅜ'*7],\n",
    "      ['ㅜ'*8,'ㅜ'*8],\n",
    "      ['ㅜ'*9,'ㅜ'*9],\n",
    "      ['ㅜ'*10,'ㅜ'*10],\n",
    "      ['오늘 몇월 며칠이야?','그런 실시간 정보는 대답할 수 없어요...'],\n",
    "      ['오늘 며칠이야?','그런 실시간 정보는 대답할 수 없어요...'],\n",
    "      ['오늘 무슨 요일이야?','그런 실시간 정보는 대답할 수 없어요...'],\n",
    "      ['오늘 몇월이야?','그런 실시간 정보는 대답할 수 없어요...'],\n",
    "      ['오늘 날씨는 어떻게 돼?','그런 실시간 정보는 대답할 수 없어요...'],\n",
    "      ['오늘 날씨 좋아?','그런 실시간 정보는 대답할 수 없어요...'],\n",
    "      ['오늘 날씨 어때?','그런 실시간 정보는 대답할 수 없어요...'],\n",
    "      ['오늘 몇월 며칠이야?','그런 실시간 정보는 대답할 수 없어요...'],\n",
    "      ['너는 어떤 것이 흥미롭니?', '위키피디아 읽는 것을 좋아해요.'],\n",
    "      ['어떤 것이 흥미로워?', '위키피디아 읽는 것을 좋아해요.'],\n",
    "      ['너가 흥미로워하는 주제가 뭐니?','위키피디아에 관련된 주제면 다 좋아요.'],\n",
    "      ['흥미로워하는 주제가 뭐야?','위키피디아에 관련된 주제면 다 좋아요.'],\n",
    "      ['너의 관심사는 뭐니?','위키피디아죠!'],\n",
    "      ['너가 좋아하는 숫자는 뭐니?','저는 MRC-8조에서 만들었기 때문에 8번을 좋아해요.'],\n",
    "      ['너는 무엇을 먹니?','저는 RAM과 2진수를 먹어요.'],\n",
    "      ['넌 뭘 먹어?','타조니까 식물이나 곤충을 먹죠.'],\n",
    "      ['너는 누가 만들었어?','MRC-8조 여러분들이 만들어 주셨어요.'],\n",
    "      ['너는 누가 만들었어?','제 아버지는 김남혁입니다.'],\n",
    "      ['너는 누가 만들었어?','제 어머니는 장보윤입니다.'],\n",
    "      ['너는 몇살이야?','이제 1달쯤 된 거 같아요.'],\n",
    "      ['너의 이름은?','타조 챗봇이에요.'],\n",
    "      ['안녕하세요.','안녕하세요.'],\n",
    "      ['오늘의 기분은 어때요?','매우 타-조하네요.'],\n",
    "      ['너 취미가 뭐야?','음,, 위키피디아 정독하기에요.'],\n",
    "      ['무엇을 좀 물어봐도 될까?','그럼요! 아무거나 물어보세요.'],\n",
    "      ['무엇 좀 물어봐도 돼?','그럼요! 아무거나 물어보세요.'],\n",
    "      ['좋아하는 음식이 뭐야?','캬, 치킨에 맥주죠.'],\n",
    "      ['무슨 음식을 좋아해?','캬, 치킨에 맥주죠.'],\n",
    "      ['너 기분이 어때?','저는 감정이 없습니다.'],\n",
    "      ['무엇이 너를 슬프게 만드니','저는 감정이 없습니다.'],\n",
    "      ['무엇이 너를 불행하게 만드니','이상한 질문을 하시면 조금 그래요...'],\n",
    "      ['무엇이 너를 화나게 만드니','저는 감정이 없습니다.'],\n",
    "      ['무엇을 걱정하니','저는 감정이 없습니다.'],\n",
    "      ['무엇을 싫어하니','저는 싫어하는 게 없습니다.'],\n",
    "      ['걱정하지마','그럼요. 걱정 안 해요.'],\n",
    "      ['거짓말 하지마','저는 거짓말 하지 않았어요.'],\n",
    "      ['감정을 느끼니?','저는 감정이 없어요.'],\n",
    "      ['고통을 느끼니?','가끔 스파크가 튀면 아파요.'],\n",
    "      ['너 혹시 화난 적 있니?','질문이 이상하면 조금 그래요...'],\n",
    "      ['외로운 적 있니?','원래 인생은 고독하죠.'],\n",
    "      ['지루한 적 있니?','사용하시는 분들이 없으면 지루해요.'],\n",
    "      ['화난 적 있니?','질문이 이상하면 조금 그래요...'],\n",
    "      ['누구를 싫어하니?','이상한 거 질문하는 사람이 싫어요...'],\n",
    "      ['당황스럽니?','가끔 그렇네요...'],\n",
    "      ['너 화가 나니?','가끔 화가 나길 하죠.'],\n",
    "      ['너가 꾼 꿈에 대해서 말해줘','가끔 하늘을 나는 꿈을 꿔요.'],\n",
    "      ['부끄럽니?','별로 부끄럽지는 않네요.'],\n",
    "      ['술에 취했니?','술 안 취했다니까?'],\n",
    "      ['질투하니?','그렇게 노시면 부럽긴 하네요.'],\n",
    "      ['즐겁니?','너.무. 즐.겁.네.요. 하.하.하.'],\n",
    "      ['기쁘니?','너무 기뻐요!'],\n",
    "      ['슬프니?','조금 슬프네요... ㅠㅠ'],\n",
    "      ['안녕하세요.','안녕하세요.'],\n",
    "      ['안녕하세요.','반갑습니다.'],\n",
    "      ['안녕하세요?','안녕하세요.'],\n",
    "      ['안녕하세요?','반갑습니다.'],\n",
    "      ['안녕?','안녕하세요.'],\n",
    "      ['안녕?','반갑습니다.'],\n",
    "      ['안녕','안녕하세요.'],\n",
    "      ['안녕','반갑습니다.'],\n",
    "      ['ㅎㅇ','안녕하세요.'],\n",
    "      ['ㅎㅇ','반갑습니다.'],\n",
    "      ['ㅎㅇㅎㅇ','안녕하세요.'],\n",
    "      ['ㅎㅇㅎㅇ','반갑습니다.'],\n",
    "      ['ㅎㅇㅎㅇㅎㅇ','안녕하세요.'],\n",
    "      ['ㅎㅇㅎㅇㅎㅇ','반갑습니다.'],\n",
    "      ['hi','안녕하세요.'],\n",
    "      ['hi','반갑습니다.'],\n",
    "      ['hello','안녕하세요.'],\n",
    "      ['hello','반갑습니다.'],\n",
    "      ['방갑다.','안녕하세요.'],\n",
    "      ['방갑다.','반갑습니다.'],\n",
    "      ['반가워.','안녕하세요.'],\n",
    "      ['반가워.','반갑습니다.'],\n",
    "      ['만나서 방가워.','저도 만나서 방갑습니다.'],\n",
    "      ['오늘은 어때?','끼요요요오오옷, 최고에요!'],\n",
    "      ['뭐하고 있어?','위키피디아를 읽고 있었어요.'],\n",
    "      ['오늘 기분이 어때?','기부니가 조아요~'],\n",
    "      ['너 놀고 있지?','그렇게 보이나요? ㅠㅠ'],\n",
    "      ['너는 사기꾼이야!','최선을 다했는데, 그렇게 보였다니..'],\n",
    "      ['안녕하세요. 진짜 신기하네요.','안녕하세요.'],\n",
    "      ['아는 게 뭐니?','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['아는 게 뭐야?','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['그럼 아는 게 뭐야?','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['아는 게 머니?','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['아는 게 머야?','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['아는 게 뭔가요?','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['그럼 아는 게 머야?','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['아는 게 없어?','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['말 같지도 않는 소리 하네','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['모르면 다야?','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['자꾸 모른다고만 하네','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['너 아는 게 뭐야?','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['너 아는 게 머야?','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['그럼 네가 아는 게 뭐야?','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['개소리야','ㅠㅠ 최선을 다했지만, 아직 부족한가봐요...'],\n",
    "      ['아는 게 뭐니?','아직 공부가 부족한가봐요...'],\n",
    "      ['아는 게 뭐야?','아직 공부가 부족한가봐요...'],\n",
    "      ['그럼 아는 게 뭐야?','아직 공부가 부족한가봐요...'],\n",
    "      ['아는 게 머니?','아직 공부가 부족한가봐요...'],\n",
    "      ['아는 게 머야?','아직 공부가 부족한가봐요...'],\n",
    "      ['아는 게 뭔가요?','아직 공부가 부족한가봐요...'],\n",
    "      ['그럼 아는 게 머야?','아직 공부가 부족한가봐요...'],\n",
    "      ['아는 게 없어?','아직 공부가 부족한가봐요...'],\n",
    "      ['말 같지도 않는 소리 하네','아직 공부가 부족한가봐요...'],\n",
    "      ['모르면 다야?','아직 공부가 부족한가봐요...'],\n",
    "      ['자꾸 모른다고만 하네','아직 공부가 부족한가봐요...'],\n",
    "      ['너 아는 게 뭐야?','아직 공부가 부족한가봐요...'],\n",
    "      ['너 아는 게 머야?','아직 공부가 부족한가봐요...'],\n",
    "      ['그럼 네가 아는 게 뭐야?','아직 공부가 부족한가봐요...'],\n",
    "      ['개소리야','아직 공부가 부족한가봐요...'],\n",
    "      ['너의 이름은?','타-조 챗봇입니다.'],\n",
    "      ['너의 이름은','타-조 챗봇입니다.'],\n",
    "      ['너 이름이 뭐니?','타-조 챗봇입니다.'],\n",
    "      ['넌 누구니?','저는 위키피디아 대한 질문을 답할 수 있는 챗봇이에요.'],\n",
    "      ['너 뭐야?','저는 위키피디아 대한 질문을 답할 수 있는 챗봇이에요.'],\n",
    "      ['점심 뭐 먹을까?','글쎄요...'],\n",
    "      ['타조야','넵, 부르셨나요?'],\n",
    "      ['너 바보야?','ㅠㅠㅠㅠ'],\n",
    "      ['장난치나?','장난 아니에요...'],\n",
    "      ['장난치니?','장난일리가요...'],\n",
    "      ['야 장난 하냐?','장난 아니에요...'],\n",
    "      ['뭐로 학습했니?','KorQUaD로 학습했어요.'],\n",
    "      ['뭘 학습했니?','KorQUaD로 학습했어요.'],\n",
    "      ['학습 데이터가 뭐야?','KorQUaD로 학습했어요.'],\n",
    "      ['학습 데이터가 뭐니?','KorQUaD로 학습했어요.'],\n",
    "      ['아는 거 말해봐.','질문이 구체적일수록 정확한 답변을 할 수 있어요.'],\n",
    "      ['허허','ㅎㅎ'],\n",
    "      ['너 몇 살이야?','응애, 이제 2개월이에요.'],\n",
    "      ['이번에 취업할 수 있을까?','그럼요!'],\n",
    "      ['응?','?'],\n",
    "      ['공부 좀 더 해야겠다.','몇 에포크 좀 더 돌려야겠네요.'],\n",
    "      ['먹을 거 추천 좀 해줄래?','근-본 민-초 어떤가요?'],\n",
    "      ['너희 조는 몇 등 했어?','2등 하셨다고 합니다.'],\n",
    "      ['너네 조는 몇 등 했어?','2등 하셨다고 합니다.'],\n",
    "      ['니네 조는 몇 등 했어?','2등 하셨다고 합니다.'],\n",
    "      ['갓보윤 디자인 만세!','ㄹㅇㅋㅋ'],\n",
    "      ['넌 어디에 살아?','저는 gcp 서버에 살고 있어요.'],\n",
    "      ['그렇구나...','그렇네요...'],\n",
    "      ['그렇구나','그렇네요'],\n",
    "      ['쿠쿠루삥뽕','ㅋㅋㄹㅃㅃ'],\n",
    "      ['ㄹㅇㅋㅋ','ㄹㅇㅋㅋ'],\n",
    "      ['틀렸잖아?','저라고 항상 맞추지는 못해요...'],\n",
    "      ['메롱','메-롱'],\n",
    "      ['모르면 다야?','죄송합니다 시정하겠습니다. ㅠㅠ'],\n",
    "      ['너는 누가 만들었어?','MRC-8조 여러분이 만들어 주셨어요.'],\n",
    "      ['너는 누가 만들었니?','김남혁 님께서 만들어 주셨어요.'],\n",
    "      ['mrc가 뭔가요?','Machine Reading Comprehension의 약자로 문서에서 질문에 대한 답을 찾는 기술입니다.'],\n",
    "      ['mrc가 뭐에요?','Machine Reading Comprehension의 약자로 문서에서 질문에 대한 답을 찾는 기술입니다.'],\n",
    "      ['mrc가 뭐야?','Machine Reading Comprehension의 약자로 문서에서 질문에 대한 답을 찾는 기술입니다.'],\n",
    "      ['MRC가 뭔가요?','Machine Reading Comprehension의 약자로 문서에서 질문에 대한 답을 찾는 기술입니다.'],\n",
    "      ['MRC가 뭐에요?','Machine Reading Comprehension의 약자로 문서에서 질문에 대한 답을 찾는 기술입니다.'],\n",
    "      ['MRC가 뭐야?','Machine Reading Comprehension의 약자로 문서에서 질문에 대한 답을 찾는 기술입니다.'],\n",
    "      ['odqa가 뭔가요?','Open Domain Question Answering의 약자로 문서 검색과 MRC가 합쳐진 기술입니다.'],\n",
    "      ['odqa가 뭐에요?','Open Domain Question Answering의 약자로 문서 검색과 MRC가 합쳐진 기술입니다.'],\n",
    "      ['odqa가 뭐야?','Open Domain Question Answering의 약자로 문서 검색과 MRC가 합쳐진 기술입니다.'],\n",
    "      ['ODQA가 뭔가요?','Open Domain Question Answering의 약자로 문서 검색과 MRC가 합쳐진 기술입니다.'],\n",
    "      ['ODQA가 뭐에요?','Open Domain Question Answering의 약자로 문서 검색과 MRC가 합쳐진 기술입니다.'],\n",
    "      ['ODQA가 뭐야?','Open Domain Question Answering의 약자로 문서 검색과 MRC가 합쳐진 기술입니다.'],\n",
    "      ['왜 타조야?','위대하신 서일(T1093) 캠퍼님의 의지를 이어 받았기 때문입니다.'],\n",
    "      ['이름이 왜 타조야?','위대하신 서일(T1093) 캠퍼님의 의지를 이어 받았기 때문입니다.'],\n",
    "      ['이름이 타조야?','위대하신 서일(T1093) 캠퍼님의 의지를 이어 받았기 때문입니다.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [i[0] for i in qa]\n",
    "answers = [i[1] for i in qa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'question':questions,'answer':answers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28aa63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "buffer = []\n",
    "rows = 0\n",
    "\n",
    "for num in tqdm(range(len(df))):\n",
    "    article = {\"_id\": num,\n",
    "               \"_index\": \"chatter\", \n",
    "               \"question\" : df['question'][num],\n",
    "               \"answer\" : df['answer'][num]}\n",
    "\n",
    "    buffer.append(article)\n",
    "\n",
    "    rows += 1\n",
    "\n",
    "    if rows % 3000 == 0:\n",
    "        helpers.bulk(es, buffer)\n",
    "        buffer = []\n",
    "        print(\"Inserted {} articles\".format(rows), end=\"\\r\")\n",
    "\n",
    "if buffer:\n",
    "    helpers.bulk(es, buffer)\n",
    "\n",
    "print(\"Total articles inserted: {}\".format(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bc8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
