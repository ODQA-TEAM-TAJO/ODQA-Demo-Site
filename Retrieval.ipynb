{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93610a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "es_server = Popen(['/home/dr_lunars/elasticsearch-7.0.0/bin/elasticsearch'],stdout=PIPE, stderr=STDOUT)\n",
    "!sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/dr_lunars/elasticsearch-7.0.0/bin/elasticsearch-plugin install analysis-nori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77641aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/dr_lunars/elasticsearch-7.0.0/bin/elasticsearch-plugin install https://github.com/javacafe-project/elasticsearch-plugin/releases/download/v7.0.0/javacafe-analyzer-7.0.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539014b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_server.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc05f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "es_server = Popen(['/home/dr_lunars/elasticsearch-7.0.0/bin/elasticsearch'],stdout=PIPE, stderr=STDOUT)\n",
    "!sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\", timeout=300, max_retries=10, retry_on_timeout=True)\n",
    "\n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f657e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.create(index = 'document',\n",
    "                  body = {\n",
    "                      'settings':{\n",
    "                          'analysis':{\n",
    "                              'analyzer':{\n",
    "                                  'my_analyzer':{\n",
    "                                      \"type\": \"custom\",\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'decompound_mode':'mixed',\n",
    "                                      'stopwords':'_korean_',\n",
    "                                      'synonyms':'_korean_',\n",
    "                                      \"filter\": [\"lowercase\",\n",
    "                                                 \"my_shingle_f\",\n",
    "                                                 \"nori_readingform\",\n",
    "                                                 \"cjk_bigram\",\n",
    "                                                 \"decimal_digit\",\n",
    "                                                 \"stemmer\",\n",
    "                                                 \"trim\"]\n",
    "                                  },\n",
    "                                  'kor2eng_analyzer':{\n",
    "                                      'type':'custom',\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'filter': [\n",
    "                                          'trim',\n",
    "                                          'lowercase',\n",
    "                                          'javacafe_kor2eng'\n",
    "                                      ]\n",
    "                                  },\n",
    "                                  'eng2kor_analyzer': {\n",
    "                                      'type': 'custom',\n",
    "                                      'tokenizer': 'nori_tokenizer',\n",
    "                                      'filter': [\n",
    "                                          'trim',\n",
    "                                          'lowercase',\n",
    "                                          'javacafe_eng2kor'\n",
    "                                      ]\n",
    "                                  },\n",
    "                              },\n",
    "                              'filter':{\n",
    "                                  'my_shingle_f':{\n",
    "                                      \"type\": \"shingle\"\n",
    "                                  }\n",
    "                              }\n",
    "                          },\n",
    "                          'similarity':{\n",
    "                              'my_similarity':{\n",
    "                                  'type':'BM25',\n",
    "                              }\n",
    "                          }\n",
    "                      },\n",
    "                      'mappings':{\n",
    "                          'properties':{\n",
    "                              'title':{\n",
    "                                  'type':'keyword',\n",
    "                                  'copy_to':['title_kor2eng','title_eng2kor']\n",
    "                              },\n",
    "                              'title_kor2eng': {\n",
    "                                  'type': 'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'search_analyzer': 'kor2eng_analyzer'\n",
    "                              },\n",
    "                              'title_eng2kor': {\n",
    "                                  'type': 'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'search_analyzer': 'eng2kor_analyzer'\n",
    "                              },\n",
    "                              'text':{\n",
    "                                  'type':'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'similarity':'my_similarity',\n",
    "                              },\n",
    "                              'text_origin': {\n",
    "                                  'type': 'text',\n",
    "                                  'analyzer': 'my_analyzer',\n",
    "                                  'similarity': 'my_similarity'\n",
    "                              }\n",
    "                          }\n",
    "                      }\n",
    "                  }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e819d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(es.indices.get('document'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe99a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m wikiextractor.WikiExtractor kowiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "title = []\n",
    "text = []\n",
    "\n",
    "for folder_path in glob('/home/dr_lunars/text/*'):\n",
    "    for file_path in glob(folder_path+'/*'):\n",
    "        f = open(file_path, 'r')\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "        for doc in data.split('</doc>'):\n",
    "            doc_split = doc.split('\">\\n')\n",
    "            try:\n",
    "                if len(doc_split[1]) >= 500:\n",
    "                    title.append(doc_split[0].split('\" title=\"')[1])\n",
    "                    text.append(doc_split[1])\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94118bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wiki_data = pd.DataFrame({'title':title,'text':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "wiki_data['text_origin'] = wiki_data['text']\n",
    "\n",
    "wiki_data['text_origin'] = wiki_data['text_origin'].apply(lambda x : ' '.join(re.sub(r'''[^ \\r\\nㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9ぁ-ゔァ-ヴー々〆〤一-龥~₩!@#$%^&*()“”‘’《》≪≫〈〉『』「」＜＞_+|{}:\"<>?`\\-=\\\\[\\];',.\\/·]''', ' ', str(x.lower().strip())).split()))\n",
    "\n",
    "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\\\n\\\\n',' '))\n",
    "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\n\\n',' '))\n",
    "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\\\n',' '))\n",
    "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\n',' '))\n",
    "\n",
    "wiki_data['text'] = wiki_data['text'].apply(lambda x : ' '.join(re.sub(r'''[^ \\r\\nㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9~₩!@#$%^&*()_+|{}:\"<>?`\\-=\\\\[\\];',.\\/]''', ' ', str(x.lower().strip())).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "title = []\n",
    "text = []\n",
    "text_origin = []\n",
    "\n",
    "for num in tqdm(range(len(wiki_data))):\n",
    "    cnt = 0\n",
    "    while cnt < len(wiki_data['text'][num]):\n",
    "        title.append(wiki_data['title'][num])\n",
    "        text.append(wiki_data['text'][num][cnt:cnt+1000])\n",
    "        text_origin.append(wiki_data['text_origin'][num])\n",
    "        cnt+=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9074cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'title':title,'text':text,'text_origin':text_origin})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ee75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "buffer = []\n",
    "rows = 0\n",
    "\n",
    "for num in tqdm(range(len(df))):\n",
    "    article = {\"_id\": num,\n",
    "               \"_index\": \"document\", \n",
    "               \"title\" : df['title'][num],\n",
    "               \"text\" : df['text'][num],\n",
    "               \"text_origin\" : df['text_origin'][num]}\n",
    "\n",
    "    buffer.append(article)\n",
    "\n",
    "    rows += 1\n",
    "\n",
    "    if rows % 3000 == 0:\n",
    "        helpers.bulk(es, buffer)\n",
    "        buffer = []\n",
    "        print(\"Inserted {} articles\".format(rows), end=\"\\r\")\n",
    "\n",
    "if buffer:\n",
    "    helpers.bulk(es, buffer)\n",
    "\n",
    "print(\"Total articles inserted: {}\".format(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.create(index = 'qa',\n",
    "                  body = {\n",
    "                      'settings':{\n",
    "                          'analysis':{\n",
    "                              'analyzer':{\n",
    "                                  'my_analyzer':{\n",
    "                                      \"type\": \"custom\",\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'decompound_mode':'mixed',\n",
    "                                      'stopwords':'_korean_',\n",
    "                                      'synonyms':'_korean_',\n",
    "                                      \"filter\": [\"lowercase\",\n",
    "                                                 \"my_shingle_f\",\n",
    "                                                 \"nori_readingform\",\n",
    "                                                 \"cjk_bigram\",\n",
    "                                                 \"decimal_digit\",\n",
    "                                                 \"stemmer\",\n",
    "                                                 \"trim\"]\n",
    "                                  },\n",
    "                                  'kor2eng_analyzer':{\n",
    "                                      'type':'custom',\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'filter': [\n",
    "                                          'trim',\n",
    "                                          'lowercase',\n",
    "                                          'javacafe_kor2eng'\n",
    "                                      ]\n",
    "                                  },\n",
    "                                  'eng2kor_analyzer': {\n",
    "                                      'type': 'custom',\n",
    "                                      'tokenizer': 'nori_tokenizer',\n",
    "                                      'filter': [\n",
    "                                          'trim',\n",
    "                                          'lowercase',\n",
    "                                          'javacafe_eng2kor'\n",
    "                                      ]\n",
    "                                  },\n",
    "                              },\n",
    "                              'filter':{\n",
    "                                  'my_shingle_f':{\n",
    "                                      \"type\": \"shingle\"\n",
    "                                  }\n",
    "                              }\n",
    "                          },\n",
    "                          'similarity':{\n",
    "                              'my_similarity':{\n",
    "                                  'type':'BM25',\n",
    "                              }\n",
    "                          }\n",
    "                      },\n",
    "                      'mappings':{\n",
    "                          'properties':{\n",
    "                              'question':{\n",
    "                                  'type':'text',\n",
    "                                  'copy_to':['question_kor2eng','question_eng2kor']\n",
    "                              },\n",
    "                              'question_kor2eng': {\n",
    "                                  'type': 'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'search_analyzer': 'kor2eng_analyzer'\n",
    "                              },\n",
    "                              'question_eng2kor': {\n",
    "                                  'type': 'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'search_analyzer': 'eng2kor_analyzer'\n",
    "                              },\n",
    "                              'answer':{\n",
    "                                  'type':'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'similarity':'my_similarity',\n",
    "                              }\n",
    "                          }\n",
    "                      }\n",
    "                  }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb87db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(es.indices.get('qa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_v1 = load_dataset('squad_kor_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f667e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"ko_wiki_v1_squad.json\", \"r\") as f:\n",
    "    ko_wiki_v1_squad = json.load(f)\n",
    "\n",
    "with open(\"ko_nia_normal_squad_all.json\", \"r\") as f:\n",
    "    ko_nia_normal_squad_all = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa54d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = dataset_v1['train']['question'] + dataset_v1['validation']['question'] + [i['paragraphs'][0]['qas'][0]['question'] for i in ko_wiki_v1_squad['data']] + [j['question'] for i in ko_nia_normal_squad_all['data'] for j in i['paragraphs'][0]['qas']]\n",
    "answers = [i['text'][0] for i in dataset_v1['train']['answers']] + [i['text'][0] for i in dataset_v1['validation']['answers']] + [i['paragraphs'][0]['qas'][0]['answers'][0]['text'] for i in ko_wiki_v1_squad['data']] + [j['answers'][0]['text'] for i in ko_nia_normal_squad_all['data'] for j in i['paragraphs'][0]['qas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'question':questions,'answer':answers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "buffer = []\n",
    "rows = 0\n",
    "\n",
    "for num in tqdm(range(len(df))):\n",
    "    article = {\"_id\": num,\n",
    "               \"_index\": \"qa\", \n",
    "               \"question\" : df['question'][num],\n",
    "               \"answer\" : df['answer'][num]}\n",
    "\n",
    "    buffer.append(article)\n",
    "\n",
    "    rows += 1\n",
    "\n",
    "    if rows % 3000 == 0:\n",
    "        helpers.bulk(es, buffer)\n",
    "        buffer = []\n",
    "        print(\"Inserted {} articles\".format(rows), end=\"\\r\")\n",
    "\n",
    "if buffer:\n",
    "    helpers.bulk(es, buffer)\n",
    "\n",
    "print(\"Total articles inserted: {}\".format(rows))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
