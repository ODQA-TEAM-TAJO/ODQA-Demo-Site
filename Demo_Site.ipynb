{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo Site.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1ba6Xpy9SLvIGow2KnhIvPYq1JEpU7_Aq",
      "authorship_tag": "ABX9TyNri+dWt7/hwPTlW+o3xpXl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skaurl/ODQA-Demo-Site/blob/main/Demo_Site.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCS7mXErsmQ4"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install elasticsearch\n",
        "!pip install flask==0.12.2\n",
        "!pip install flask-ngrok\n",
        "!pip install konlpy\n",
        "!pip install pororo\n",
        "!pip install python-mecab-ko"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG7WOUEIs_D9"
      },
      "source": [
        "!git clone https://github.com/skaurl/ODQA-Demo-Site.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbNpkDHps9-q"
      },
      "source": [
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.0.0-linux-x86_64.tar.gz -q\n",
        "!tar -xzf elasticsearch-7.0.0-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.0.0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojeNfK_ZsmXA"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJqiNR_xsmcq"
      },
      "source": [
        "!cd Mecab-ko-for-Google-Colab"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzP-0w0AslzX"
      },
      "source": [
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiY_5o79tU1P"
      },
      "source": [
        "!cd content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G4gXX_lSH2A"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['/content/elasticsearch-7.0.0/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)\n",
        "                  )\n",
        "!sleep 30"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOMqhkKASHy3"
      },
      "source": [
        "!/content/elasticsearch-7.0.0/bin/elasticsearch-plugin install analysis-nori"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYlr115xSHvj"
      },
      "source": [
        "!/content/elasticsearch-7.0.0/bin/elasticsearch-plugin install https://github.com/javacafe-project/elasticsearch-plugin/releases/download/v7.0.0/javacafe-analyzer-7.0.0.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fae0g0xfSHr4"
      },
      "source": [
        "es_server.kill()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNjzESL4SHoF"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['/content/elasticsearch-7.0.0/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)\n",
        "                  )\n",
        "!sleep 30"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-2Cx36JSHkw"
      },
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "es = Elasticsearch('localhost:9200')\n",
        "\n",
        "print(es.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3FHyxGPS0Lj"
      },
      "source": [
        "es.indices.create(index = 'document',\n",
        "                  body = {\n",
        "                      'settings':{\n",
        "                          'analysis':{\n",
        "                              'analyzer':{\n",
        "                                  'my_analyzer':{\n",
        "                                      \"type\": \"custom\",\n",
        "                                      'tokenizer':'nori_tokenizer',\n",
        "                                      'decompound_mode':'mixed',\n",
        "                                      'stopwords':'_korean_',\n",
        "                                      'synonyms':'_korean_',\n",
        "                                      \"filter\": [\"lowercase\",\n",
        "                                                 \"my_shingle_f\",\n",
        "                                                 \"nori_readingform\",\n",
        "                                                 \"cjk_bigram\",\n",
        "                                                 \"decimal_digit\",\n",
        "                                                 \"stemmer\",\n",
        "                                                 \"trim\"]\n",
        "                                  },\n",
        "                                  'kor2eng_analyzer':{\n",
        "                                      'type':'custom',\n",
        "                                      'tokenizer':'nori_tokenizer',\n",
        "                                      'filter': [\n",
        "                                          'trim',\n",
        "                                          'lowercase',\n",
        "                                          'javacafe_kor2eng'\n",
        "                                      ]\n",
        "                                  },\n",
        "                                  'eng2kor_analyzer': {\n",
        "                                      'type': 'custom',\n",
        "                                      'tokenizer': 'nori_tokenizer',\n",
        "                                      'filter': [\n",
        "                                          'trim',\n",
        "                                          'lowercase',\n",
        "                                          'javacafe_eng2kor'\n",
        "                                      ]\n",
        "                                  },\n",
        "                              },\n",
        "                              'filter':{\n",
        "                                  'my_shingle_f':{\n",
        "                                      \"type\": \"shingle\"\n",
        "                                  }\n",
        "                              }\n",
        "                          },\n",
        "                          'similarity':{\n",
        "                              'my_similarity':{\n",
        "                                  'type':'BM25',\n",
        "                              }\n",
        "                          }\n",
        "                      },\n",
        "                      'mappings':{\n",
        "                          'properties':{\n",
        "                              'title':{\n",
        "                                  'type':'keyword',\n",
        "                                  'copy_to':['title_kor2eng','title_eng2kor']\n",
        "                              },\n",
        "                              'title_kor2eng': {\n",
        "                                  'type': 'text',\n",
        "                                  'analyzer':'my_analyzer',\n",
        "                                  'search_analyzer': 'kor2eng_analyzer'\n",
        "                              },\n",
        "                              'title_eng2kor': {\n",
        "                                  'type': 'text',\n",
        "                                  'analyzer':'my_analyzer',\n",
        "                                  'search_analyzer': 'eng2kor_analyzer'\n",
        "                              },\n",
        "                              'text':{\n",
        "                                  'type':'text',\n",
        "                                  'analyzer':'my_analyzer',\n",
        "                                  'similarity':'my_similarity',\n",
        "                              },\n",
        "                              'text_origin': {\n",
        "                                  'type': 'text',\n",
        "                                  'analyzer': 'my_analyzer',\n",
        "                                  'similarity': 'my_similarity'\n",
        "                              }\n",
        "                          }\n",
        "                      }\n",
        "                  }\n",
        "                  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iu_gZnmTCTe"
      },
      "source": [
        "print(es.indices.get('document'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13L2pYSKTMHw"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('/content/drive/MyDrive/한양대학교/부스트캠프 AI Tech/[P3] 기계독해/wikipedia_documents.json', 'r') as f:\n",
        "    wiki_data = pd.DataFrame(json.load(f)).transpose()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o_IbxbcTMBl"
      },
      "source": [
        "wiki_data = wiki_data.drop_duplicates(['text']) # 3876\n",
        "\n",
        "wiki_data = wiki_data.reset_index()\n",
        "\n",
        "del wiki_data['index']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTssuekWWv6N"
      },
      "source": [
        "import re\n",
        "\n",
        "wiki_data['text_origin'] = wiki_data['text']\n",
        "\n",
        "wiki_data['text_origin'] = wiki_data['text_origin'].apply(lambda x : ' '.join(re.sub(r'''[^ \\r\\nㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9ぁ-ゔァ-ヴー々〆〤一-龥~₩!@#$%^&*()“”‘’《》≪≫〈〉『』「」＜＞_+|{}:\"<>?`\\-=\\\\[\\];',.\\/·]''', ' ', str(x.lower().strip())).split()))\n",
        "\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\\\n\\\\n',' '))\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\n\\n',' '))\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\\\n',' '))\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\n',' '))\n",
        "\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : ' '.join(re.sub(r'''[^ \\r\\nㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9~₩!@#$%^&*()_+|{}:\"<>?`\\-=\\\\[\\];',.\\/]''', ' ', str(x.lower().strip())).split()))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlQoyPLBWwB-"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "title = []\n",
        "text = []\n",
        "text_origin = []\n",
        "\n",
        "for num in tqdm(range(len(wiki_data))):\n",
        "    cnt = 0\n",
        "    while cnt < len(wiki_data['text'][num]):\n",
        "        title.append(wiki_data['title'][num])\n",
        "        text.append(wiki_data['text'][num][cnt:cnt+1000])\n",
        "        text_origin.append(wiki_data['text_origin'][num])\n",
        "        cnt+=1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU3fttbiWwJm"
      },
      "source": [
        "df = pd.DataFrame({'title':title,'text':text,'text_origin':text_origin})"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skdg5N_xWwNb"
      },
      "source": [
        "from elasticsearch import Elasticsearch, helpers\n",
        "\n",
        "buffer = []\n",
        "rows = 0\n",
        "\n",
        "for num in tqdm(range(len(df))):\n",
        "    article = {\"_id\": num,\n",
        "               \"_index\": \"document\", \n",
        "               \"title\" : df['title'][num],\n",
        "               \"text\" : df['text'][num],\n",
        "               \"text_origin\" : df['text_origin'][num]}\n",
        "\n",
        "    buffer.append(article)\n",
        "\n",
        "    rows += 1\n",
        "\n",
        "    if rows % 3000 == 0:\n",
        "        helpers.bulk(es, buffer)\n",
        "        buffer = []\n",
        "\n",
        "        print(\"Inserted {} articles\".format(rows), end=\"\\r\")\n",
        "\n",
        "if buffer:\n",
        "    helpers.bulk(es, buffer)\n",
        "\n",
        "print(\"Total articles inserted: {}\".format(rows))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78ykg2T7TSs-"
      },
      "source": [
        "from typing import Optional, Dict, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from fairseq.models.roberta import RobertaHubInterface, RobertaModel\n",
        "\n",
        "from pororo.models.brainbert.utils import softmax\n",
        "from pororo.tasks.utils.download_utils import download_or_load\n",
        "from pororo.tasks.utils.tokenizer import CustomTokenizer\n",
        "from pororo.tasks.utils.base import PororoBiencoderBase, PororoFactoryBase"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Emx1aFwTSv7"
      },
      "source": [
        "class PororoMrcFactory(PororoFactoryBase):\n",
        "\n",
        "    def __init__(self, task: str, lang: str, model: Optional[str]):\n",
        "        super().__init__(task, lang, model)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_available_langs():\n",
        "        return [\"ko\"]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_available_models():\n",
        "        return {\"ko\": [\"brainbert.base.ko.korquad\"]}\n",
        "\n",
        "    def load(self, device: str):\n",
        "\n",
        "        if \"brainbert\" in self.config.n_model:\n",
        "            try:\n",
        "                import mecab\n",
        "            except ModuleNotFoundError as error:\n",
        "                raise error.__class__(\n",
        "                    \"Please install python-mecab-ko with: `pip install python-mecab-ko`\"\n",
        "                )\n",
        "\n",
        "            from pororo.utils import postprocess_span\n",
        "\n",
        "            model = (My_BrainRobertaModel.load_model(\n",
        "                f\"bert/{self.config.n_model}\",\n",
        "                self.config.lang,\n",
        "            ).eval().to(device))\n",
        "\n",
        "            tagger = mecab.MeCab()\n",
        "\n",
        "            return PororoBertMrc(model, tagger, postprocess_span, self.config)\n",
        "\n",
        "class My_BrainRobertaModel(RobertaModel):\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, model_name: str, lang: str, **kwargs):\n",
        "\n",
        "        from fairseq import hub_utils\n",
        "\n",
        "        ckpt_dir = download_or_load(model_name, lang)\n",
        "        tok_path = download_or_load(f\"tokenizers/bpe32k.{lang}.zip\", lang)\n",
        "\n",
        "        x = hub_utils.from_pretrained(\n",
        "            ckpt_dir,\n",
        "            \"model.pt\",\n",
        "            ckpt_dir,\n",
        "            load_checkpoint_heads=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "        return BrainRobertaHubInterface(\n",
        "            x[\"args\"],\n",
        "            x[\"task\"],\n",
        "            x[\"models\"][0],\n",
        "            tok_path,\n",
        "        )"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PIoCwN3TS1a"
      },
      "source": [
        "class BrainRobertaHubInterface(RobertaHubInterface):\n",
        "\n",
        "    def __init__(self, args, task, model, tok_path):\n",
        "        super().__init__(args, task, model)\n",
        "        self.bpe = CustomTokenizer.from_file(\n",
        "            vocab_filename=f\"{tok_path}/vocab.json\",\n",
        "            merges_filename=f\"{tok_path}/merges.txt\",\n",
        "        )\n",
        "\n",
        "    def tokenize(self, sentence: str, add_special_tokens: bool = False):\n",
        "        result = \" \".join(self.bpe.encode(sentence).tokens)\n",
        "        if add_special_tokens:\n",
        "            result = f\"<s> {result} </s>\"\n",
        "        return result\n",
        "\n",
        "    def encode(\n",
        "        self,\n",
        "        sentence: str,\n",
        "        *addl_sentences,\n",
        "        add_special_tokens: bool = True,\n",
        "        no_separator: bool = False,\n",
        "    ) -> torch.LongTensor:\n",
        "\n",
        "        bpe_sentence = self.tokenize(\n",
        "            sentence,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "        )\n",
        "\n",
        "        for s in addl_sentences:\n",
        "            bpe_sentence += \" </s>\" if not no_separator and add_special_tokens else \"\"\n",
        "            bpe_sentence += (\" \" + self.tokenize(s, add_special_tokens=False) +\n",
        "                             \" </s>\" if add_special_tokens else \"\")\n",
        "        tokens = self.task.source_dictionary.encode_line(\n",
        "            bpe_sentence,\n",
        "            append_eos=False,\n",
        "            add_if_not_exist=False,\n",
        "        )\n",
        "        return tokens.long()\n",
        "\n",
        "    def decode(\n",
        "        self,\n",
        "        tokens: torch.LongTensor,\n",
        "        skip_special_tokens: bool = True,\n",
        "        remove_bpe: bool = True,\n",
        "    ) -> str:\n",
        "        assert tokens.dim() == 1\n",
        "        tokens = tokens.numpy()\n",
        "\n",
        "        if tokens[0] == self.task.source_dictionary.bos(\n",
        "        ) and skip_special_tokens:\n",
        "            tokens = tokens[1:]\n",
        "\n",
        "        eos_mask = tokens == self.task.source_dictionary.eos()\n",
        "        doc_mask = eos_mask[1:] & eos_mask[:-1]\n",
        "        sentences = np.split(tokens, doc_mask.nonzero()[0] + 1)\n",
        "\n",
        "        if skip_special_tokens:\n",
        "            sentences = [\n",
        "                np.array(\n",
        "                    [c\n",
        "                     for c in s\n",
        "                     if c != self.task.source_dictionary.eos()])\n",
        "                for s in sentences\n",
        "            ]\n",
        "\n",
        "        sentences = [\n",
        "            \" \".join([self.task.source_dictionary.symbols[c]\n",
        "                      for c in s])\n",
        "            for s in sentences\n",
        "        ]\n",
        "\n",
        "        if remove_bpe:\n",
        "            sentences = [\n",
        "                s.replace(\" \", \"\").replace(\"▁\", \" \").strip() for s in sentences\n",
        "            ]\n",
        "        if len(sentences) == 1:\n",
        "            return sentences[0]\n",
        "        return sentences\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_span(\n",
        "        self,\n",
        "        question: str,\n",
        "        context: str,\n",
        "        add_special_tokens: bool = True,\n",
        "        no_separator: bool = False,\n",
        "    ) -> Tuple:\n",
        "\n",
        "        max_length = self.task.max_positions()\n",
        "        tokens = self.encode(\n",
        "            question,\n",
        "            context,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            no_separator=no_separator,\n",
        "        )[:max_length]\n",
        "        with torch.no_grad():\n",
        "            logits = self.predict(\n",
        "                \"span_prediction_head\",\n",
        "                tokens,\n",
        "                return_logits=True,\n",
        "            ).squeeze()\n",
        "\n",
        "            results = []\n",
        "\n",
        "            top_n = 10\n",
        "            \n",
        "            starts = logits[:,0].argsort(descending = True)[:top_n].tolist()\n",
        "\n",
        "            for start in starts:\n",
        "                ends = logits[:,1].argsort(descending = True).tolist()\n",
        "                masked_ends = [end for end in ends if end >= start ]\n",
        "                ends = (masked_ends+ends)[:top_n]\n",
        "                for end in ends:\n",
        "                    answer_tokens = tokens[start:end + 1]\n",
        "                    answer = \"\"\n",
        "                    if len(answer_tokens) >= 1:\n",
        "                        decoded = self.decode(answer_tokens)\n",
        "                        if isinstance(decoded, str):\n",
        "                            answer = decoded\n",
        "\n",
        "                    score = ((logits[:,0][start] + 5) * (logits[:,1][end] + 5)).item()\n",
        "                    results.append((answer, (start, end + 1), score))\n",
        "\n",
        "            ends = logits[:,1].argsort(descending = True)[:top_n].tolist()\n",
        "\n",
        "            for end in ends:\n",
        "                starts = logits[:,0].argsort(descending = True).tolist()\n",
        "                masked_starts = [start for start in starts if start >= end ]\n",
        "                starts = (masked_starts+starts)[:top_n]\n",
        "                for start in starts:\n",
        "                    answer_tokens = tokens[start:end + 1]\n",
        "                    answer = \"\"\n",
        "                    if len(answer_tokens) >= 1:\n",
        "                        decoded = self.decode(answer_tokens)\n",
        "                        if isinstance(decoded, str):\n",
        "                            answer = decoded\n",
        "\n",
        "                    score = ((logits[:,0][start] + 5) * (logits[:,1][end] + 5)).item()\n",
        "                    results.append((answer, (start, end + 1), score))\n",
        "            \n",
        "        return results"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozjFxW1eTS6w"
      },
      "source": [
        "class PororoBertMrc(PororoBiencoderBase):\n",
        "\n",
        "    def __init__(self, model, tagger, callback, config):\n",
        "        super().__init__(config)\n",
        "        self._model = model\n",
        "        self._tagger = tagger\n",
        "        self._callback = callback\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        query: str,\n",
        "        context: str,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[str, Tuple[int, int]]:\n",
        "\n",
        "        postprocess = kwargs.get(\"postprocess\", True)\n",
        "\n",
        "        pair_results = self._model.predict_span(query, context)\n",
        "        returns = []\n",
        "        \n",
        "        for pair_result in pair_results:\n",
        "            span = self._callback(\n",
        "            self._tagger,\n",
        "            pair_result[0],\n",
        "            ) if postprocess else pair_result[0]\n",
        "            returns.append((span,pair_result[1],pair_result[2]))\n",
        "        \n",
        "        return returns"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7dZWruFTTCp"
      },
      "source": [
        "from pororo import Pororo\n",
        "ner = Pororo(task=\"ner\", lang=\"ko\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awaNWQNqSHZR"
      },
      "source": [
        "mrc_factory = PororoMrcFactory('mrc', 'ko', \"brainbert.base.ko.korquad\")\n",
        "mrc = mrc_factory.load(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zUQkWxokHZg"
      },
      "source": [
        "from flask import Flask, render_template, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__,static_folder='/content/ODQA-Demo-Site/static',template_folder='/content/ODQA-Demo-Site/templates')\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/get\")\n",
        "def get_bot_response():\n",
        "    question = request.args.get('msg')\n",
        "    query = {\n",
        "        'query':{\n",
        "            'bool':{\n",
        "                'must':[\n",
        "                        {'match':{'text':question}}\n",
        "                ],\n",
        "                'should':[\n",
        "                        {'match':{'text':' '.join([i[0] for i in ner(question) if i[1] != 'O'])}}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    doc = es.search(index='document',body=query,size=10)['hits']['hits']\n",
        "    ans_lst = []\n",
        "    max_scr = doc[0]['_score']\n",
        "    for i in range(len(doc)):\n",
        "        ans = mrc(question,doc[0]['_source']['text'],postprocess=False)[0]\n",
        "        if ans == '' or 'unk' in ans or len(ans) >= 30:\n",
        "            pass\n",
        "        else:\n",
        "            ans_lst.append((ans,ans[1],ans[2]*doc[i]['_score']/max_scr))\n",
        "    res = sorted(ans_lst, key = lambda x : x[2], reverse=True)[0][0][0]\n",
        "    return res\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}