{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo Site.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xHtGyJv_Kttg",
        "gQJT-envK7F0",
        "GqLN6reXLEc5",
        "_-jt9JsBLZI5"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1ba6Xpy9SLvIGow2KnhIvPYq1JEpU7_Aq",
      "authorship_tag": "ABX9TyOjpa+/FoLucssEOqlc2lMA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skaurl/ODQA-Demo-Site/blob/main/Demo_Site.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHtGyJv_Kttg"
      },
      "source": [
        "# Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCS7mXErsmQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78b55de-21db-4a26-bbca-fd02aa39b400"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install elasticsearch\n",
        "!pip install flask==0.12.2\n",
        "!pip install flask-ngrok\n",
        "!pip install konlpy\n",
        "!pip install pororo\n",
        "!pip install python-mecab-ko"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/f8/ff7cd6e3b400b33dcbbfd31c6c1481678a2b2f669f521ad20053009a9aa3/datasets-1.7.0-py3-none-any.whl (234kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20kB 25.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 51kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61kB 17.2MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 112kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 133kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 153kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 174kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 194kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 204kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 215kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 225kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235kB 14.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.0.1)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/32/a1/7c5261396da23ec364e296a4fb8a1cd6a5a2ff457215c6447038f18c0309/huggingface_hub-0.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 20.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: huggingface-hub, fsspec, xxhash, datasets\n",
            "Successfully installed datasets-1.7.0 fsspec-2021.5.0 huggingface-hub-0.0.9 xxhash-2.0.2\n",
            "Collecting elasticsearch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/b1/58cfb0bf54e29c20669d6e588496fb7fe8b54f53bc238be4cb0a185a1e76/elasticsearch-7.13.1-py2.py3-none-any.whl (354kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (2020.12.5)\n",
            "Installing collected packages: elasticsearch\n",
            "Successfully installed elasticsearch-7.13.1\n",
            "Collecting flask==0.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/32/e3597cb19ffffe724ad4bf0beca4153419918e7fa4ba6a34b04ee4da3371/Flask-0.12.2-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (1.1.0)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.4->flask==0.12.2) (2.0.1)\n",
            "Installing collected packages: flask\n",
            "  Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "Successfully installed flask-0.12.2\n",
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (0.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.4->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.4MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Collecting pororo\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/ab/f409aab13ba2a4e2576d2ea4b877396029c617d17553edbbb9ba64cf4ee9/pororo-0.4.2-py3-none-any.whl (256kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 14.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pororo) (1.0.1)\n",
            "Collecting word2word\n",
            "  Downloading https://files.pythonhosted.org/packages/0b/27/f6e5252e44fbf10678189aa7c03dfaec44942d2cd593cb957263862a650a/word2word-1.0.0-py3-none-any.whl\n",
            "Collecting sentence-transformers>=0.4.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/75/df441011cd1726822b70fbff50042adb4860e9327b99b346154ead704c44/sentence-transformers-1.2.0.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.8MB/s \n",
            "\u001b[?25hCollecting torchvision==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/b5/60d5eb61f1880707a5749fea43e0ec76f27dfe69391cdec953ab5da5e676/torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.9MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9MB 14.7MB/s \n",
            "\u001b[?25hCollecting marisa-trie\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/95/d23071d0992dabcb61c948fb118a90683193befc88c23e745b050a29e7db/marisa-trie-0.7.5.tar.gz (270kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from pororo) (7.1.2)\n",
            "Collecting transformers>=4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 49.0MB/s \n",
            "\u001b[?25hCollecting whoosh\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/19/24d0f1f454a2c1eb689ca28d2f178db81e5024f42d82729a4ff6771155cf/Whoosh-2.7.4-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 50.2MB/s \n",
            "\u001b[?25hCollecting fairseq>=0.10.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/ab/92c6efb05ffdfe16fbdc9e463229d9af8c3b74dc943ed4b4857a87b223c2/fairseq-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 47.9MB/s \n",
            "\u001b[?25hCollecting nltk>=3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pororo) (4.2.6)\n",
            "Collecting g2p-en\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/d9/b77dc634a7a0c0c97716ba97dd0a28cbfa6267c96f359c4f27ed71cbd284/g2p_en-2.1.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 42.9MB/s \n",
            "\u001b[?25hCollecting torch==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/5e/35140615fc1f925023f489e71086a9ecc188053d263d3594237281284d82/torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8MB 21kB/s \n",
            "\u001b[?25hCollecting kss\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/ea/3030770642a58a08777dfa324a1b65a2f53f1574de8dd84424851f0c2ec7/kss-2.5.1-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.4MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from word2word->pororo) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from word2word->pororo) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from word2word->pororo) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1.2->pororo) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1.2->pororo) (1.4.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->pororo) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->pororo) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->pororo) (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->pororo) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 40.8MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 35.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq>=0.10.2->pororo) (1.14.5)\n",
            "Collecting hydra-core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e3/fbd70dd0d3ce4d1d75c22d56c0c9f895cfa7ed6587a9ffb821d6812d6a60/hydra_core-1.0.6-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 57.5MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq>=0.10.2->pororo) (0.29.23)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.5->pororo) (7.1.2)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from g2p-en->pororo) (2.1.0)\n",
            "Collecting distance>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/1a/883e47df323437aefa0d0a92ccfb38895d9416bd0b56262c2e46a47767b8/Distance-0.1.3.tar.gz (180kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->pororo) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->word2word->pororo) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->word2word->pororo) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->word2word->pororo) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->word2word->pororo) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=4.0.0->pororo) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=4.0.0->pororo) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=4.0.0->pororo) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->pororo) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq>=0.10.2->pororo) (2.20)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 50.4MB/s \n",
            "\u001b[?25hCollecting omegaconf<2.1,>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq>=0.10.2->pororo) (5.1.3)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 56.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: sentence-transformers, marisa-trie, wget, distance, antlr4-python3-runtime\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.2.0-cp37-none-any.whl size=123339 sha256=91362a55c79d9950debc8daac8fa0281f7d6b74d3aef25f0a54c6eccd19d0b09\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/06/f7/faaa96fdda87462b4fd5c47b343340e9d5531ef70d0eef8242\n",
            "  Building wheel for marisa-trie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for marisa-trie: filename=marisa_trie-0.7.5-cp37-cp37m-linux_x86_64.whl size=860934 sha256=4592c831aa4f45defda7c4c4f52b7c2f18d3bdc4a3bce76d39983852bd30364f\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/24/79/022624fc914f0e559fe8a1141aaff1f9df810905a13fc75d57\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9675 sha256=01f1dcda49e71cc2e9512b3a8eeab1b81445364e2139b2d3233c57a3b2cd4e0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-cp37-none-any.whl size=16275 sha256=0d205ee0fdf9f7e61ec608ed423dde29c012b5bcfa9c3c37fe61a330bc35f655\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/aa/e1/dbba9e7b6d397d645d0f12db1c66dbae9c5442b39b001db18e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=ac50e2dd5273e7ae302e8962302e7db9317c00d6739deca2b421b07ff3e87623\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built sentence-transformers marisa-trie wget distance antlr4-python3-runtime\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: wget, word2word, huggingface-hub, sacremoses, tokenizers, transformers, torch, torchvision, nltk, sentencepiece, sentence-transformers, marisa-trie, whoosh, antlr4-python3-runtime, PyYAML, omegaconf, hydra-core, dataclasses, portalocker, sacrebleu, fairseq, distance, g2p-en, kss, pororo\n",
            "  Found existing installation: huggingface-hub 0.0.9\n",
            "    Uninstalling huggingface-hub-0.0.9:\n",
            "      Successfully uninstalled huggingface-hub-0.0.9\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 dataclasses-0.6 distance-0.1.3 fairseq-0.10.2 g2p-en-2.1.0 huggingface-hub-0.0.8 hydra-core-1.0.6 kss-2.5.1 marisa-trie-0.7.5 nltk-3.6.2 omegaconf-2.0.6 pororo-0.4.2 portalocker-2.0.0 sacrebleu-1.5.1 sacremoses-0.0.45 sentence-transformers-1.2.0 sentencepiece-0.1.95 tokenizers-0.10.3 torch-1.6.0 torchvision-0.7.0 transformers-4.6.1 wget-3.2 whoosh-2.7.4 word2word-1.0.0\n",
            "Collecting python-mecab-ko\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/1d/9c869b1230dfd12c2fc84cfd307ae76a48f8e218db19feab00ef451a147e/python-mecab-ko-1.0.11.tar.gz\n",
            "Collecting pybind11~=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/43/7339dbabbc2793718d59703aace4166f53c29ee1c202f6ff5bf8a26c4d91/pybind11-2.6.2-py2.py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 15.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: python-mecab-ko\n",
            "  Building wheel for python-mecab-ko (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for python-mecab-ko\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for python-mecab-ko\n",
            "Failed to build python-mecab-ko\n",
            "Installing collected packages: pybind11, python-mecab-ko\n",
            "    Running setup.py install for python-mecab-ko ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed pybind11-2.6.2 python-mecab-ko-1.0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG7WOUEIs_D9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e893acc6-56ad-4302-e41f-45f90f2f669b"
      },
      "source": [
        "!git clone https://github.com/skaurl/ODQA-Demo-Site.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ODQA-Demo-Site'...\n",
            "remote: Enumerating objects: 197, done.\u001b[K\n",
            "remote: Counting objects: 100% (197/197), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 197 (delta 68), reused 182 (delta 59), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (197/197), 878.36 KiB | 7.98 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbNpkDHps9-q"
      },
      "source": [
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.0.0-linux-x86_64.tar.gz -q\n",
        "!tar -xzf elasticsearch-7.0.0-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.0.0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojeNfK_ZsmXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0541a5-9b52-400c-8364-7cbd5ced90f2"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJqiNR_xsmcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844a8a6b-c80a-4a9f-e3c3-60a42f1fd4d7"
      },
      "source": [
        "cd Mecab-ko-for-Google-Colab"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Mecab-ko-for-Google-Colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzP-0w0AslzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ef2934-b232-44ef-d2f0-b4a07aa6f8b1"
      },
      "source": [
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.2.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2021-06-05 09:49:38--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c2:513, 2406:da00:ff00::6b17:d1f5, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=YHc0h1mfMc4lkOSqF1vzZ1VPB90%3D&Expires=1622887593&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-06-05 09:49:38--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=YHc0h1mfMc4lkOSqF1vzZ1VPB90%3D&Expires=1622887593&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.133.75\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.133.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  2.62MB/s    in 0.5s    \n",
            "\n",
            "2021-06-05 09:49:39 (2.62 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2021-06-05 09:50:52--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c2:513, 2406:da00:ff00::22c3:9b0a, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=zXsr4%2FuaSj%2Fye1wBPC2Oz%2BkNLW8%3D&Expires=1622888296&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-06-05 09:50:52--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=zXsr4%2FuaSj%2Fye1wBPC2Oz%2BkNLW8%3D&Expires=1622888296&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.65.84\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.65.84|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  23.0MB/s    in 2.1s    \n",
            "\n",
            "2021-06-05 09:50:55 (23.0 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiY_5o79tU1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04eba5e-fa66-4fc3-d1a6-b7824bc3af2d"
      },
      "source": [
        "cd /content"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQJT-envK7F0"
      },
      "source": [
        "# Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G4gXX_lSH2A"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['/content/elasticsearch-7.0.0/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)\n",
        "                  )\n",
        "!sleep 30"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOMqhkKASHy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e5cfb0c-3aea-41e2-e3d9-65753db46e4f"
      },
      "source": [
        "!/content/elasticsearch-7.0.0/bin/elasticsearch-plugin install analysis-nori"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-> Downloading analysis-nori from elastic\n",
            "[=================================================] 100%   \n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.bouncycastle.jcajce.provider.drbg.DRBG (file:/content/elasticsearch-7.0.0/lib/tools/plugin-cli/bcprov-jdk15on-1.61.jar) to constructor sun.security.provider.Sun()\n",
            "WARNING: Please consider reporting this to the maintainers of org.bouncycastle.jcajce.provider.drbg.DRBG\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "-> Installed analysis-nori\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYlr115xSHvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f4224be-c5fd-40b8-d16e-7531b874f713"
      },
      "source": [
        "!/content/elasticsearch-7.0.0/bin/elasticsearch-plugin install https://github.com/javacafe-project/elasticsearch-plugin/releases/download/v7.0.0/javacafe-analyzer-7.0.0.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-> Downloading https://github.com/javacafe-project/elasticsearch-plugin/releases/download/v7.0.0/javacafe-analyzer-7.0.0.zip\n",
            "[=================================================] 100%   \n",
            "-> Installed javacafe-analyzer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fae0g0xfSHr4"
      },
      "source": [
        "es_server.kill()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNjzESL4SHoF"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['/content/elasticsearch-7.0.0/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)\n",
        "                  )\n",
        "!sleep 30"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-2Cx36JSHkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b72c3990-e2f4-4808-945e-2e1bfcb66910"
      },
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "es = Elasticsearch('localhost:9200')\n",
        "\n",
        "print(es.info())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': '1d2dd50128fc', 'cluster_name': 'elasticsearch', 'cluster_uuid': 'J8WLAVqsToGseDCl3bPIHQ', 'version': {'number': '7.0.0', 'build_flavor': 'default', 'build_type': 'tar', 'build_hash': 'b7e28a7', 'build_date': '2019-04-05T22:55:32.697037Z', 'build_snapshot': False, 'lucene_version': '8.0.0', 'minimum_wire_compatibility_version': '6.7.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3FHyxGPS0Lj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff947a1d-1409-4fc8-bc33-846ec2ad3c4e"
      },
      "source": [
        "es.indices.create(index = 'document',\n",
        "                  body = {\n",
        "                      'settings':{\n",
        "                          'analysis':{\n",
        "                              'analyzer':{\n",
        "                                  'my_analyzer':{\n",
        "                                      \"type\": \"custom\",\n",
        "                                      'tokenizer':'nori_tokenizer',\n",
        "                                      'decompound_mode':'mixed',\n",
        "                                      'stopwords':'_korean_',\n",
        "                                      'synonyms':'_korean_',\n",
        "                                      \"filter\": [\"lowercase\",\n",
        "                                                 \"my_shingle_f\",\n",
        "                                                 \"nori_readingform\",\n",
        "                                                 \"cjk_bigram\",\n",
        "                                                 \"decimal_digit\",\n",
        "                                                 \"stemmer\",\n",
        "                                                 \"trim\"]\n",
        "                                  },\n",
        "                                  'kor2eng_analyzer':{\n",
        "                                      'type':'custom',\n",
        "                                      'tokenizer':'nori_tokenizer',\n",
        "                                      'filter': [\n",
        "                                          'trim',\n",
        "                                          'lowercase',\n",
        "                                          'javacafe_kor2eng'\n",
        "                                      ]\n",
        "                                  },\n",
        "                                  'eng2kor_analyzer': {\n",
        "                                      'type': 'custom',\n",
        "                                      'tokenizer': 'nori_tokenizer',\n",
        "                                      'filter': [\n",
        "                                          'trim',\n",
        "                                          'lowercase',\n",
        "                                          'javacafe_eng2kor'\n",
        "                                      ]\n",
        "                                  },\n",
        "                              },\n",
        "                              'filter':{\n",
        "                                  'my_shingle_f':{\n",
        "                                      \"type\": \"shingle\"\n",
        "                                  }\n",
        "                              }\n",
        "                          },\n",
        "                          'similarity':{\n",
        "                              'my_similarity':{\n",
        "                                  'type':'BM25',\n",
        "                              }\n",
        "                          }\n",
        "                      },\n",
        "                      'mappings':{\n",
        "                          'properties':{\n",
        "                              'title':{\n",
        "                                  'type':'keyword',\n",
        "                                  'copy_to':['title_kor2eng','title_eng2kor']\n",
        "                              },\n",
        "                              'title_kor2eng': {\n",
        "                                  'type': 'text',\n",
        "                                  'analyzer':'my_analyzer',\n",
        "                                  'search_analyzer': 'kor2eng_analyzer'\n",
        "                              },\n",
        "                              'title_eng2kor': {\n",
        "                                  'type': 'text',\n",
        "                                  'analyzer':'my_analyzer',\n",
        "                                  'search_analyzer': 'eng2kor_analyzer'\n",
        "                              },\n",
        "                              'text':{\n",
        "                                  'type':'text',\n",
        "                                  'analyzer':'my_analyzer',\n",
        "                                  'similarity':'my_similarity',\n",
        "                              },\n",
        "                              'text_origin': {\n",
        "                                  'type': 'text',\n",
        "                                  'analyzer': 'my_analyzer',\n",
        "                                  'similarity': 'my_similarity'\n",
        "                              }\n",
        "                          }\n",
        "                      }\n",
        "                  }\n",
        "                  )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acknowledged': True, 'index': 'document', 'shards_acknowledged': True}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iu_gZnmTCTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a58530f-343e-4efc-91bc-a7fe7c33efa7"
      },
      "source": [
        "print(es.indices.get('document'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'document': {'aliases': {}, 'mappings': {'properties': {'text': {'type': 'text', 'similarity': 'my_similarity', 'analyzer': 'my_analyzer'}, 'text_origin': {'type': 'text', 'similarity': 'my_similarity', 'analyzer': 'my_analyzer'}, 'title': {'type': 'keyword', 'copy_to': ['title_kor2eng', 'title_eng2kor']}, 'title_eng2kor': {'type': 'text', 'analyzer': 'my_analyzer', 'search_analyzer': 'eng2kor_analyzer'}, 'title_kor2eng': {'type': 'text', 'analyzer': 'my_analyzer', 'search_analyzer': 'kor2eng_analyzer'}}}, 'settings': {'index': {'number_of_shards': '1', 'provided_name': 'document', 'similarity': {'my_similarity': {'type': 'BM25'}}, 'creation_date': '1622886842658', 'analysis': {'filter': {'my_shingle_f': {'type': 'shingle'}}, 'analyzer': {'eng2kor_analyzer': {'filter': ['trim', 'lowercase', 'javacafe_eng2kor'], 'type': 'custom', 'tokenizer': 'nori_tokenizer'}, 'kor2eng_analyzer': {'filter': ['trim', 'lowercase', 'javacafe_kor2eng'], 'type': 'custom', 'tokenizer': 'nori_tokenizer'}, 'my_analyzer': {'filter': ['lowercase', 'my_shingle_f', 'nori_readingform', 'cjk_bigram', 'decimal_digit', 'stemmer', 'trim'], 'decompound_mode': 'mixed', 'type': 'custom', 'stopwords': '_korean_', 'synonyms': '_korean_', 'tokenizer': 'nori_tokenizer'}}}, 'number_of_replicas': '1', 'uuid': 'rdIxRQM9TvOs2MSeCYZqlg', 'version': {'created': '7000099'}}}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13L2pYSKTMHw"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('/content/drive/MyDrive/한양대학교/부스트캠프 AI Tech/[P3] 기계독해/wikipedia_documents.json', 'r') as f:\n",
        "    wiki_data = pd.DataFrame(json.load(f)).transpose()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o_IbxbcTMBl"
      },
      "source": [
        "wiki_data = wiki_data.drop_duplicates(['text']) # 3876\n",
        "\n",
        "wiki_data = wiki_data.reset_index()\n",
        "\n",
        "del wiki_data['index']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTssuekWWv6N"
      },
      "source": [
        "import re\n",
        "\n",
        "wiki_data['text_origin'] = wiki_data['text']\n",
        "\n",
        "wiki_data['text_origin'] = wiki_data['text_origin'].apply(lambda x : ' '.join(re.sub(r'''[^ \\r\\nㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9ぁ-ゔァ-ヴー々〆〤一-龥~₩!@#$%^&*()“”‘’《》≪≫〈〉『』「」＜＞_+|{}:\"<>?`\\-=\\\\[\\];',.\\/·]''', ' ', str(x.lower().strip())).split()))\n",
        "\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\\\n\\\\n',' '))\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\n\\n',' '))\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\\\n',' '))\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\n',' '))\n",
        "\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : ' '.join(re.sub(r'''[^ \\r\\nㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9~₩!@#$%^&*()_+|{}:\"<>?`\\-=\\\\[\\];',.\\/]''', ' ', str(x.lower().strip())).split()))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlQoyPLBWwB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e8e3fbc-f0f7-49f4-fdbe-0099dee98e6f"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "title = []\n",
        "text = []\n",
        "text_origin = []\n",
        "\n",
        "for num in tqdm(range(len(wiki_data))):\n",
        "    cnt = 0\n",
        "    while cnt < len(wiki_data['text'][num]):\n",
        "        title.append(wiki_data['title'][num])\n",
        "        text.append(wiki_data['text'][num][cnt:cnt+1000])\n",
        "        text_origin.append(wiki_data['text_origin'][num])\n",
        "        cnt+=1000"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 56737/56737 [00:02<00:00, 28335.49it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU3fttbiWwJm"
      },
      "source": [
        "df = pd.DataFrame({'title':title,'text':text,'text_origin':text_origin})"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skdg5N_xWwNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f693b80-fecf-4c7f-9196-b3795878a469"
      },
      "source": [
        "from elasticsearch import Elasticsearch, helpers\n",
        "\n",
        "buffer = []\n",
        "rows = 0\n",
        "\n",
        "for num in tqdm(range(len(df))):\n",
        "    article = {\"_id\": num,\n",
        "               \"_index\": \"document\", \n",
        "               \"title\" : df['title'][num],\n",
        "               \"text\" : df['text'][num],\n",
        "               \"text_origin\" : df['text_origin'][num]}\n",
        "\n",
        "    buffer.append(article)\n",
        "\n",
        "    rows += 1\n",
        "\n",
        "    if rows % 3000 == 0:\n",
        "        helpers.bulk(es, buffer)\n",
        "        buffer = []\n",
        "        print(\"Inserted {} articles\".format(rows), end=\"\\r\")\n",
        "\n",
        "if buffer:\n",
        "    helpers.bulk(es, buffer)\n",
        "\n",
        "print(\"Total articles inserted: {}\".format(rows))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 3000/69555 [00:14<05:12, 212.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 3000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|▊         | 6000/69555 [00:31<05:16, 200.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 6000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 9000/69555 [00:47<05:08, 196.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 9000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 12000/69555 [00:57<04:22, 218.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 12000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 15000/69555 [01:11<04:11, 216.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 15000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 26%|██▌       | 18000/69555 [01:21<03:37, 237.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 18000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 21000/69555 [01:33<03:25, 236.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 21000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 35%|███▍      | 24000/69555 [01:44<03:01, 250.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 24000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 39%|███▉      | 27000/69555 [01:57<02:55, 242.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 27000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 30000/69555 [02:07<02:34, 256.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 30000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 33000/69555 [02:20<02:26, 248.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 33000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 52%|█████▏    | 36000/69555 [02:34<02:19, 239.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 36000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 39000/69555 [02:45<02:02, 248.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 39000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 42000/69555 [02:55<01:45, 260.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 42000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 65%|██████▍   | 45000/69555 [03:05<01:29, 273.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 45000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 69%|██████▉   | 48000/69555 [03:17<01:21, 264.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 48000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 51000/69555 [03:26<01:06, 280.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 51000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 54000/69555 [03:41<01:01, 252.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 54000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 57000/69555 [03:50<00:46, 270.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 57000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 86%|████████▋ | 60000/69555 [04:00<00:34, 277.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 60000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 91%|█████████ | 63000/69555 [04:09<00:22, 289.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 63000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|█████████▍| 66000/69555 [04:20<00:12, 290.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 66000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 69555/69555 [04:33<00:00, 254.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 69000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total articles inserted: 69555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqLN6reXLEc5"
      },
      "source": [
        "# MRC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7dZWruFTTCp"
      },
      "source": [
        "from typing import Optional, Dict, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from fairseq.models.roberta import RobertaHubInterface, RobertaModel\n",
        "\n",
        "from pororo.models.brainbert.utils import softmax\n",
        "from pororo.tasks.utils.download_utils import download_or_load\n",
        "from pororo.tasks.utils.tokenizer import CustomTokenizer\n",
        "from pororo.tasks.utils.base import PororoBiencoderBase, PororoFactoryBase"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNUFgs1wfQkn"
      },
      "source": [
        "class PororoMrcFactory(PororoFactoryBase):\n",
        "\n",
        "    def __init__(self, task: str, lang: str, model: Optional[str]):\n",
        "        super().__init__(task, lang, model)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_available_langs():\n",
        "        return [\"ko\"]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_available_models():\n",
        "        return {\"ko\": [\"brainbert.base.ko.korquad\"]}\n",
        "\n",
        "    def load(self, device: str):\n",
        "\n",
        "        if \"brainbert\" in self.config.n_model:\n",
        "            try:\n",
        "                import mecab\n",
        "            except ModuleNotFoundError as error:\n",
        "                raise error.__class__(\n",
        "                    \"Please install python-mecab-ko with: `pip install python-mecab-ko`\"\n",
        "                )\n",
        "\n",
        "            from pororo.utils import postprocess_span\n",
        "\n",
        "            model = (My_BrainRobertaModel.load_model(\n",
        "                f\"bert/{self.config.n_model}\",\n",
        "                self.config.lang,\n",
        "            ).eval().to(device))\n",
        "\n",
        "            tagger = mecab.MeCab()\n",
        "\n",
        "            return PororoBertMrc(model, tagger, postprocess_span, self.config)\n",
        "\n",
        "class My_BrainRobertaModel(RobertaModel):\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, model_name: str, lang: str, **kwargs):\n",
        "\n",
        "        from fairseq import hub_utils\n",
        "\n",
        "        ckpt_dir = download_or_load(model_name, lang)\n",
        "        tok_path = download_or_load(f\"tokenizers/bpe32k.{lang}.zip\", lang)\n",
        "\n",
        "        x = hub_utils.from_pretrained(\n",
        "            ckpt_dir,\n",
        "            \"model.pt\",\n",
        "            ckpt_dir,\n",
        "            load_checkpoint_heads=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "        return BrainRobertaHubInterface(\n",
        "            x[\"args\"],\n",
        "            x[\"task\"],\n",
        "            x[\"models\"][0],\n",
        "            tok_path,\n",
        "        )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIVlCMF4fSur"
      },
      "source": [
        "class BrainRobertaHubInterface(RobertaHubInterface):\n",
        "\n",
        "    def __init__(self, args, task, model, tok_path):\n",
        "        super().__init__(args, task, model)\n",
        "        self.bpe = CustomTokenizer.from_file(\n",
        "            vocab_filename=f\"{tok_path}/vocab.json\",\n",
        "            merges_filename=f\"{tok_path}/merges.txt\",\n",
        "        )\n",
        "\n",
        "    def tokenize(self, sentence: str, add_special_tokens: bool = False):\n",
        "        result = \" \".join(self.bpe.encode(sentence).tokens)\n",
        "        if add_special_tokens:\n",
        "            result = f\"<s> {result} </s>\"\n",
        "        return result\n",
        "\n",
        "    def encode(\n",
        "        self,\n",
        "        sentence: str,\n",
        "        *addl_sentences,\n",
        "        add_special_tokens: bool = True,\n",
        "        no_separator: bool = False,\n",
        "    ) -> torch.LongTensor:\n",
        "\n",
        "        bpe_sentence = self.tokenize(\n",
        "            sentence,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "        )\n",
        "\n",
        "        for s in addl_sentences:\n",
        "            bpe_sentence += \" </s>\" if not no_separator and add_special_tokens else \"\"\n",
        "            bpe_sentence += (\" \" + self.tokenize(s, add_special_tokens=False) +\n",
        "                             \" </s>\" if add_special_tokens else \"\")\n",
        "        tokens = self.task.source_dictionary.encode_line(\n",
        "            bpe_sentence,\n",
        "            append_eos=False,\n",
        "            add_if_not_exist=False,\n",
        "        )\n",
        "        return tokens.long()\n",
        "\n",
        "    def decode(\n",
        "        self,\n",
        "        tokens: torch.LongTensor,\n",
        "        skip_special_tokens: bool = True,\n",
        "        remove_bpe: bool = True,\n",
        "    ) -> str:\n",
        "        assert tokens.dim() == 1\n",
        "        tokens = tokens.numpy()\n",
        "\n",
        "        if tokens[0] == self.task.source_dictionary.bos(\n",
        "        ) and skip_special_tokens:\n",
        "            tokens = tokens[1:]\n",
        "\n",
        "        eos_mask = tokens == self.task.source_dictionary.eos()\n",
        "        doc_mask = eos_mask[1:] & eos_mask[:-1]\n",
        "        sentences = np.split(tokens, doc_mask.nonzero()[0] + 1)\n",
        "\n",
        "        if skip_special_tokens:\n",
        "            sentences = [\n",
        "                np.array(\n",
        "                    [c\n",
        "                     for c in s\n",
        "                     if c != self.task.source_dictionary.eos()])\n",
        "                for s in sentences\n",
        "            ]\n",
        "\n",
        "        sentences = [\n",
        "            \" \".join([self.task.source_dictionary.symbols[c]\n",
        "                      for c in s])\n",
        "            for s in sentences\n",
        "        ]\n",
        "\n",
        "        if remove_bpe:\n",
        "            sentences = [\n",
        "                s.replace(\" \", \"\").replace(\"▁\", \" \").strip() for s in sentences\n",
        "            ]\n",
        "        if len(sentences) == 1:\n",
        "            return sentences[0]\n",
        "        return sentences\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_span(\n",
        "        self,\n",
        "        question: str,\n",
        "        context: str,\n",
        "        add_special_tokens: bool = True,\n",
        "        no_separator: bool = False,\n",
        "    ) -> Tuple:\n",
        "\n",
        "        max_length = self.task.max_positions()\n",
        "        tokens = self.encode(\n",
        "            question,\n",
        "            context,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            no_separator=no_separator,\n",
        "        )[:max_length]\n",
        "        with torch.no_grad():\n",
        "            logits = self.predict(\n",
        "                \"span_prediction_head\",\n",
        "                tokens,\n",
        "                return_logits=True,\n",
        "            ).squeeze()\n",
        "\n",
        "            results = []\n",
        "\n",
        "            top_n = 10\n",
        "            \n",
        "            starts = logits[:,0].argsort(descending = True)[:top_n].tolist()\n",
        "\n",
        "            for start in starts:\n",
        "                ends = logits[:,1].argsort(descending = True).tolist()\n",
        "                masked_ends = [end for end in ends if end >= start ]\n",
        "                ends = (masked_ends+ends)[:top_n]\n",
        "                for end in ends:\n",
        "                    answer_tokens = tokens[start:end + 1]\n",
        "                    answer = \"\"\n",
        "                    if len(answer_tokens) >= 1:\n",
        "                        decoded = self.decode(answer_tokens)\n",
        "                        if isinstance(decoded, str):\n",
        "                            answer = decoded\n",
        "\n",
        "                    score = ((logits[:,0][start] + 5) * (logits[:,1][end] + 5)).item()\n",
        "                    results.append((answer, (start, end + 1), score))\n",
        "\n",
        "            ends = logits[:,1].argsort(descending = True)[:top_n].tolist()\n",
        "\n",
        "            for end in ends:\n",
        "                starts = logits[:,0].argsort(descending = True).tolist()\n",
        "                masked_starts = [start for start in starts if start >= end ]\n",
        "                starts = (masked_starts+starts)[:top_n]\n",
        "                for start in starts:\n",
        "                    answer_tokens = tokens[start:end + 1]\n",
        "                    answer = \"\"\n",
        "                    if len(answer_tokens) >= 1:\n",
        "                        decoded = self.decode(answer_tokens)\n",
        "                        if isinstance(decoded, str):\n",
        "                            answer = decoded\n",
        "\n",
        "                    score = ((logits[:,0][start] + 5) * (logits[:,1][end] + 5)).item()\n",
        "                    results.append((answer, (start, end + 1), score))\n",
        "            \n",
        "        return results"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJUdyeAwfUew"
      },
      "source": [
        "class PororoBertMrc(PororoBiencoderBase):\n",
        "\n",
        "    def __init__(self, model, tagger, callback, config):\n",
        "        super().__init__(config)\n",
        "        self._model = model\n",
        "        self._tagger = tagger\n",
        "        self._callback = callback\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        query: str,\n",
        "        context: str,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[str, Tuple[int, int]]:\n",
        "\n",
        "        postprocess = kwargs.get(\"postprocess\", True)\n",
        "\n",
        "        pair_results = self._model.predict_span(query, context)\n",
        "        returns = []\n",
        "        \n",
        "        for pair_result in pair_results:\n",
        "            span = self._callback(\n",
        "            self._tagger,\n",
        "            pair_result[0],\n",
        "            ) if postprocess else pair_result[0]\n",
        "            returns.append((span,pair_result[1],pair_result[2]))\n",
        "        \n",
        "        return returns"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEr4RpqrfbCz",
        "outputId": "a34a0375-f2be-44b4-a167-137133062e62"
      },
      "source": [
        "mrc_factory = PororoMrcFactory('mrc', 'ko', \"brainbert.base.ko.korquad\")\n",
        "mrc = mrc_factory.load(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-jt9JsBLZI5"
      },
      "source": [
        "# Postprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4mq2x7XB8_E"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Hannanum\n",
        "\n",
        "mecab = Mecab()\n",
        "kkma = Kkma()\n",
        "hannanum = Hannanum()\n",
        "\n",
        "# 'JC','JX','JKS','JKC','JKG','JKO','JKB','JKV','JKQ','EP','EF','EC','ETN','ETM'\n",
        "\n",
        "def postprocess(ans):\n",
        "    if mecab.pos(ans)[-1][-1] in [\"JX\", \"JKB\", \"JKO\", \"JKS\", \"ETM\", \"VCP\", \"JC\"]:\n",
        "        ans = ans[:-len(mecab.pos(ans)[-1][0])]\n",
        "    elif ans[-1] == \"의\":\n",
        "        if kkma.pos(ans)[-1][-1] == \"JKG\" or mecab.pos(ans)[-1][-1] == \"NNG\" or hannanum.pos(ans)[-1][-1] == \"J\":\n",
        "            ans = ans[:-1]\n",
        "    return ans"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82Pw6waifdy5",
        "outputId": "54882dc3-7db8-4275-9150-1bcbbdc2a743"
      },
      "source": [
        "from pororo import Pororo\n",
        "ner = Pororo(task=\"ner\", lang=\"ko\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lTuWAylLPnG"
      },
      "source": [
        "# Flask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zUQkWxokHZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aee631e-80f1-4ef3-e767-ee7482fcf0fc"
      },
      "source": [
        "from flask import Flask, render_template, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__,static_folder='/content/ODQA-Demo-Site/static',template_folder='/content/ODQA-Demo-Site/templates')\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/get\")\n",
        "def get_bot_response():\n",
        "    question = request.args.get('msg')\n",
        "    \n",
        "    for i in ['안녕','ㅎㅇ']:\n",
        "        if i in question.lower():\n",
        "            return '안녕하세요!'\n",
        "\n",
        "    query = {\n",
        "        'query':{\n",
        "            'bool':{\n",
        "                'must':[\n",
        "                        {'match':{'text':question}}\n",
        "                ],\n",
        "                'should':[\n",
        "                        {'match':{'text':' '.join([i[0] for i in ner(question) if i[1] != 'O'])}}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    doc = es.search(index='document',body=query,size=10)['hits']['hits']\n",
        "    ans_lst = []\n",
        "    max_scr = doc[0]['_score']\n",
        "    for i in range(len(doc)):\n",
        "        ans = mrc(question,doc[i]['_source']['text'],postprocess=False)[0]\n",
        "        if ans[0] not in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = ''\n",
        "        else:\n",
        "            ans_tmp = ans[0]\n",
        "        if ans_tmp != '':\n",
        "            ans_tmp = postprocess(ans_tmp)\n",
        "        else:\n",
        "            ans_tmp = ''\n",
        "        if ans_tmp.count('(') != ans_tmp.count(')'):\n",
        "            ans_tmp = ans_tmp.replace('(','')\n",
        "            ans_tmp = ans_tmp.replace(')','')\n",
        "        if ans_tmp == '':\n",
        "            pass\n",
        "        elif \"'\" + ans_tmp + \"'\" in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = \"'\" + ans_tmp + \"'\"\n",
        "        elif '\"' + ans_tmp + '\"' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '\"' + ans_tmp + '\"'\n",
        "        elif '(' + ans_tmp + ')' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '(' + ans_tmp + ')'\n",
        "        elif '“' + ans_tmp + '”' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '“' + ans_tmp + '”'\n",
        "        elif '‘' + ans_tmp + '’' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '‘' + ans_tmp + '’'\n",
        "        elif '《' + ans_tmp + '》' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '《' + ans_tmp + '》'\n",
        "        elif '≪' + ans_tmp + '≫' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '≪' + ans_tmp + '≫'\n",
        "        elif '〈' + ans_tmp + '〉' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '〈' + ans_tmp + '〉'\n",
        "        elif '『' + ans_tmp + '』' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '『' + ans_tmp + '』'\n",
        "        elif '「' + ans_tmp + '」' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '「' + ans_tmp + '」'\n",
        "        elif '＜' + ans_tmp + '＞' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '＜' + ans_tmp + '＞'\n",
        "        elif '{' + ans_tmp + '}' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '{' + ans_tmp + '}'\n",
        "        elif '<' + ans_tmp + '>' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '<' + ans_tmp + '>'\n",
        "        elif '[' + ans_tmp + ']' in doc[i]['_source']['text_origin']:\n",
        "            ans_tmp = '[' + ans_tmp + ']'\n",
        "        try:\n",
        "            if ans_tmp != '':\n",
        "                p = re.compile(ans_tmp + \"\\([ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9ぁ-ゔァ-ヴー々〆〤一-龥]*\\)\")\n",
        "                m = p.findall(doc[i]['_source']['text_origin'])\n",
        "\n",
        "                if len(m) != 0:\n",
        "                    ans_tmp = m[0]\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            if ans_tmp != '':\n",
        "                p = re.compile(ans_tmp + \"\\s\\([ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9ぁ-ゔァ-ヴー々〆〤一-龥]*\\)\")\n",
        "                m = p.findall(doc[i]['_source']['text_origin'])\n",
        "\n",
        "                if len(m) != 0:\n",
        "                    ans_tmp = m[0]\n",
        "        except:\n",
        "            pass\n",
        "        if ans_tmp == '' or 'unk' in ans_tmp or len(ans_tmp) >= 30:\n",
        "            pass\n",
        "        else:\n",
        "            ans_lst.append((ans_tmp,ans[2]*doc[i]['_score']/max_scr))\n",
        "    res = sorted(ans_lst, key = lambda x : x[1], reverse=True)[0]\n",
        "    if res[1] >= 100:\n",
        "        return res[0] + ' 입니다.'\n",
        "    else:\n",
        "        return '죄송해요... 잘 모르겠어요...'\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://a2071bdb8758.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}