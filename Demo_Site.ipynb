{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo Site.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1ba6Xpy9SLvIGow2KnhIvPYq1JEpU7_Aq",
      "authorship_tag": "ABX9TyNri+dWt7/hwPTlW+o3xpXl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skaurl/ODQA-Demo-Site/blob/main/Demo_Site.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCS7mXErsmQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce3c9dd-aef4-4ae7-fbc5-af6bcc324e50"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install elasticsearch\n",
        "!pip install flask==0.12.2\n",
        "!pip install flask-ngrok\n",
        "!pip install konlpy\n",
        "!pip install pororo\n",
        "!pip install python-mecab-ko"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/f8/ff7cd6e3b400b33dcbbfd31c6c1481678a2b2f669f521ad20053009a9aa3/datasets-1.7.0-py3-none-any.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 2.8MB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/32/a1/7c5261396da23ec364e296a4fb8a1cd6a5a2ff457215c6447038f18c0309/huggingface_hub-0.0.9-py3-none-any.whl\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: fsspec, huggingface-hub, xxhash, datasets\n",
            "Successfully installed datasets-1.7.0 fsspec-2021.5.0 huggingface-hub-0.0.9 xxhash-2.0.2\n",
            "Collecting elasticsearch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/94/8af17aa11ea60dac63adbe51ca097263e4c4a164a44d7044632d21ee5d0c/elasticsearch-7.13.0-py2.py3-none-any.whl (354kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (1.24.3)\n",
            "Installing collected packages: elasticsearch\n",
            "Successfully installed elasticsearch-7.13.0\n",
            "Collecting flask==0.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/32/e3597cb19ffffe724ad4bf0beca4153419918e7fa4ba6a34b04ee4da3371/Flask-0.12.2-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (2.11.3)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.7/dist-packages (from flask==0.12.2) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.4->flask==0.12.2) (2.0.1)\n",
            "Installing collected packages: flask\n",
            "  Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "Successfully installed flask-0.12.2\n",
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (0.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.4->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 68.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.2MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 63.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Collecting pororo\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/ab/f409aab13ba2a4e2576d2ea4b877396029c617d17553edbbb9ba64cf4ee9/pororo-0.4.2-py3-none-any.whl (256kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 2.9MB/s \n",
            "\u001b[?25hCollecting transformers>=4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 12.0MB/s \n",
            "\u001b[?25hCollecting fairseq>=0.10.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/ab/92c6efb05ffdfe16fbdc9e463229d9af8c3b74dc943ed4b4857a87b223c2/fairseq-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 20.8MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pororo) (1.0.1)\n",
            "Collecting sentence-transformers>=0.4.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/75/df441011cd1726822b70fbff50042adb4860e9327b99b346154ead704c44/sentence-transformers-1.2.0.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.3MB/s \n",
            "\u001b[?25hCollecting nltk>=3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 26.4MB/s \n",
            "\u001b[?25hCollecting torch==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/5e/35140615fc1f925023f489e71086a9ecc188053d263d3594237281284d82/torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8MB 23kB/s \n",
            "\u001b[?25hCollecting torchvision==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/b5/60d5eb61f1880707a5749fea43e0ec76f27dfe69391cdec953ab5da5e676/torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.9MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9MB 30.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pororo) (4.2.6)\n",
            "Collecting whoosh\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/19/24d0f1f454a2c1eb689ca28d2f178db81e5024f42d82729a4ff6771155cf/Whoosh-2.7.4-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 40.5MB/s \n",
            "\u001b[?25hCollecting kss\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/ea/3030770642a58a08777dfa324a1b65a2f53f1574de8dd84424851f0c2ec7/kss-2.5.1-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hCollecting marisa-trie\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/95/d23071d0992dabcb61c948fb118a90683193befc88c23e745b050a29e7db/marisa-trie-0.7.5.tar.gz (270kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 67.3MB/s \n",
            "\u001b[?25hCollecting word2word\n",
            "  Downloading https://files.pythonhosted.org/packages/0b/27/f6e5252e44fbf10678189aa7c03dfaec44942d2cd593cb957263862a650a/word2word-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from pororo) (7.1.2)\n",
            "Collecting g2p-en\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/d9/b77dc634a7a0c0c97716ba97dd0a28cbfa6267c96f359c4f27ed71cbd284/g2p_en-2.1.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 32.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 49.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->pororo) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->pororo) (1.19.5)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->pororo) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->pororo) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->pororo) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->pororo) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->pororo) (3.0.12)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.2MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq>=0.10.2->pororo) (1.14.5)\n",
            "Collecting hydra-core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e3/fbd70dd0d3ce4d1d75c22d56c0c9f895cfa7ed6587a9ffb821d6812d6a60/hydra_core-1.0.6-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq>=0.10.2->pororo) (0.29.23)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1.2->pororo) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1.2->pororo) (1.4.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.5->pororo) (7.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->pororo) (0.16.0)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from g2p-en->pororo) (2.1.0)\n",
            "Collecting distance>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/1a/883e47df323437aefa0d0a92ccfb38895d9416bd0b56262c2e46a47767b8/Distance-0.1.3.tar.gz (180kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 56.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->pororo) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=4.0.0->pororo) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=4.0.0->pororo) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=4.0.0->pororo) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.0.0->pororo) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.0.0->pororo) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.0.0->pororo) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.0.0->pororo) (2.10)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq>=0.10.2->pororo) (2.20)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq>=0.10.2->pororo) (5.1.3)\n",
            "Collecting omegaconf<2.1,>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 40.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wget, sentence-transformers, marisa-trie, distance, antlr4-python3-runtime\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=4eae3b2e5712eb94d8d986311679992e1d3a9a3372c20492edaae4c8eaf883f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.2.0-cp37-none-any.whl size=123339 sha256=e708a4154650f1ad381591f8a4a71e8fb4662e0bf1ddec276e66e631ad9b63e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/06/f7/faaa96fdda87462b4fd5c47b343340e9d5531ef70d0eef8242\n",
            "  Building wheel for marisa-trie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for marisa-trie: filename=marisa_trie-0.7.5-cp37-cp37m-linux_x86_64.whl size=860714 sha256=f814dc24d02e30995d4920fb392db4f1934b3d3fb9357b5b985d00c4c8b88cb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/24/79/022624fc914f0e559fe8a1141aaff1f9df810905a13fc75d57\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-cp37-none-any.whl size=16261 sha256=806b4750720aeaeeb7c57728d4f635a8ca845ff2e6885ca994eaa331bfa02341\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/aa/e1/dbba9e7b6d397d645d0f12db1c66dbae9c5442b39b001db18e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=6f5481289cef7e665b3474ea7cf524efef603f65027419e0fa540b3ae50dd187\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built wget sentence-transformers marisa-trie distance antlr4-python3-runtime\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers, portalocker, sacrebleu, dataclasses, antlr4-python3-runtime, PyYAML, omegaconf, hydra-core, torch, fairseq, wget, torchvision, nltk, sentencepiece, sentence-transformers, whoosh, kss, marisa-trie, word2word, distance, g2p-en, pororo\n",
            "  Found existing installation: huggingface-hub 0.0.9\n",
            "    Uninstalling huggingface-hub-0.0.9:\n",
            "      Successfully uninstalled huggingface-hub-0.0.9\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 dataclasses-0.6 distance-0.1.3 fairseq-0.10.2 g2p-en-2.1.0 huggingface-hub-0.0.8 hydra-core-1.0.6 kss-2.5.1 marisa-trie-0.7.5 nltk-3.6.2 omegaconf-2.0.6 pororo-0.4.2 portalocker-2.0.0 sacrebleu-1.5.1 sacremoses-0.0.45 sentence-transformers-1.2.0 sentencepiece-0.1.95 tokenizers-0.10.3 torch-1.6.0 torchvision-0.7.0 transformers-4.6.1 wget-3.2 whoosh-2.7.4 word2word-1.0.0\n",
            "Collecting python-mecab-ko\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/1d/9c869b1230dfd12c2fc84cfd307ae76a48f8e218db19feab00ef451a147e/python-mecab-ko-1.0.11.tar.gz\n",
            "Collecting pybind11~=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/43/7339dbabbc2793718d59703aace4166f53c29ee1c202f6ff5bf8a26c4d91/pybind11-2.6.2-py2.py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: python-mecab-ko\n",
            "  Building wheel for python-mecab-ko (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for python-mecab-ko\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for python-mecab-ko\n",
            "Failed to build python-mecab-ko\n",
            "Installing collected packages: pybind11, python-mecab-ko\n",
            "    Running setup.py install for python-mecab-ko ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed pybind11-2.6.2 python-mecab-ko-1.0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG7WOUEIs_D9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91140b8d-1d64-4b4c-fd3c-a95435c103bd"
      },
      "source": [
        "!git clone https://github.com/skaurl/ODQA-Demo-Site.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ODQA-Demo-Site'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Counting objects: 100% (126/126), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 126 (delta 33), reused 116 (delta 29), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (126/126), 831.22 KiB | 4.05 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbNpkDHps9-q"
      },
      "source": [
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.0.0-linux-x86_64.tar.gz -q\n",
        "!tar -xzf elasticsearch-7.0.0-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.0.0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojeNfK_ZsmXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d14737-473a-4b0f-c5a1-9099cda2b73f"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJqiNR_xsmcq"
      },
      "source": [
        "!cd Mecab-ko-for-Google-Colab"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzP-0w0AslzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3445015d-3a6f-4a84-821a-993ea1ce3fcf"
      },
      "source": [
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bash: install_mecab-ko_on_colab190912.sh: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiY_5o79tU1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ae5b00-ee80-4240-b4f8-0a7301d6d24b"
      },
      "source": [
        "!cd content"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: cd: content: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G4gXX_lSH2A"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['/content/elasticsearch-7.0.0/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)\n",
        "                  )\n",
        "!sleep 30"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOMqhkKASHy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7340ca7-8e32-4f2d-d80c-460dd184ba86"
      },
      "source": [
        "!/content/elasticsearch-7.0.0/bin/elasticsearch-plugin install analysis-nori"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-> Downloading analysis-nori from elastic\n",
            "[=================================================] 100%   \n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.bouncycastle.jcajce.provider.drbg.DRBG (file:/content/elasticsearch-7.0.0/lib/tools/plugin-cli/bcprov-jdk15on-1.61.jar) to constructor sun.security.provider.Sun()\n",
            "WARNING: Please consider reporting this to the maintainers of org.bouncycastle.jcajce.provider.drbg.DRBG\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "-> Installed analysis-nori\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYlr115xSHvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a7224d6-fcbb-48b2-f445-9af41af0071f"
      },
      "source": [
        "!/content/elasticsearch-7.0.0/bin/elasticsearch-plugin install https://github.com/javacafe-project/elasticsearch-plugin/releases/download/v7.0.0/javacafe-analyzer-7.0.0.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-> Downloading https://github.com/javacafe-project/elasticsearch-plugin/releases/download/v7.0.0/javacafe-analyzer-7.0.0.zip\n",
            "[=================================================] 100%   \n",
            "-> Installed javacafe-analyzer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fae0g0xfSHr4"
      },
      "source": [
        "es_server.kill()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNjzESL4SHoF"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['/content/elasticsearch-7.0.0/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)\n",
        "                  )\n",
        "!sleep 30"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-2Cx36JSHkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876efd18-e52a-47c3-8d5e-ff91605be8de"
      },
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "es = Elasticsearch('localhost:9200')\n",
        "\n",
        "print(es.info())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': '3e9a5bf00940', 'cluster_name': 'elasticsearch', 'cluster_uuid': 'fcZv8bAPTYWKFmBu0Aze3g', 'version': {'number': '7.0.0', 'build_flavor': 'default', 'build_type': 'tar', 'build_hash': 'b7e28a7', 'build_date': '2019-04-05T22:55:32.697037Z', 'build_snapshot': False, 'lucene_version': '8.0.0', 'minimum_wire_compatibility_version': '6.7.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3FHyxGPS0Lj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffdaad8-3bb6-421c-dd34-4a6474958d65"
      },
      "source": [
        "es.indices.create(index = 'document',\n",
        "                  body = {\n",
        "                      'settings':{\n",
        "                          'analysis':{\n",
        "                              'analyzer':{\n",
        "                                  'my_analyzer':{\n",
        "                                      \"type\": \"custom\",\n",
        "                                      'tokenizer':'nori_tokenizer',\n",
        "                                      'decompound_mode':'mixed',\n",
        "                                      'stopwords':'_korean_',\n",
        "                                      'synonyms':'_korean_',\n",
        "                                      \"filter\": [\"lowercase\",\n",
        "                                                 \"my_shingle_f\",\n",
        "                                                 \"nori_readingform\",\n",
        "                                                 \"cjk_bigram\",\n",
        "                                                 \"decimal_digit\",\n",
        "                                                 \"stemmer\",\n",
        "                                                 \"trim\"]\n",
        "                                  },\n",
        "                                  'kor2eng_analyzer':{\n",
        "                                      'type':'custom',\n",
        "                                      'tokenizer':'nori_tokenizer',\n",
        "                                      'filter': [\n",
        "                                          'trim',\n",
        "                                          'lowercase',\n",
        "                                          'javacafe_kor2eng'\n",
        "                                      ]\n",
        "                                  },\n",
        "                                  'eng2kor_analyzer': {\n",
        "                                      'type': 'custom',\n",
        "                                      'tokenizer': 'nori_tokenizer',\n",
        "                                      'filter': [\n",
        "                                          'trim',\n",
        "                                          'lowercase',\n",
        "                                          'javacafe_eng2kor'\n",
        "                                      ]\n",
        "                                  },\n",
        "                              },\n",
        "                              'filter':{\n",
        "                                  'my_shingle_f':{\n",
        "                                      \"type\": \"shingle\"\n",
        "                                  }\n",
        "                              }\n",
        "                          },\n",
        "                          'similarity':{\n",
        "                              'my_similarity':{\n",
        "                                  'type':'BM25',\n",
        "                              }\n",
        "                          }\n",
        "                      },\n",
        "                      'mappings':{\n",
        "                          'properties':{\n",
        "                              'title':{\n",
        "                                  'type':'keyword',\n",
        "                                  'copy_to':['title_kor2eng','title_eng2kor']\n",
        "                              },\n",
        "                              'title_kor2eng': {\n",
        "                                  'type': 'text',\n",
        "                                  'analyzer':'my_analyzer',\n",
        "                                  'search_analyzer': 'kor2eng_analyzer'\n",
        "                              },\n",
        "                              'title_eng2kor': {\n",
        "                                  'type': 'text',\n",
        "                                  'analyzer':'my_analyzer',\n",
        "                                  'search_analyzer': 'eng2kor_analyzer'\n",
        "                              },\n",
        "                              'text':{\n",
        "                                  'type':'text',\n",
        "                                  'analyzer':'my_analyzer',\n",
        "                                  'similarity':'my_similarity',\n",
        "                              },\n",
        "                              'text_origin': {\n",
        "                                  'type': 'text',\n",
        "                                  'analyzer': 'my_analyzer',\n",
        "                                  'similarity': 'my_similarity'\n",
        "                              }\n",
        "                          }\n",
        "                      }\n",
        "                  }\n",
        "                  )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acknowledged': True, 'index': 'document', 'shards_acknowledged': True}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iu_gZnmTCTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945e6773-9d69-47fd-c8f5-10cefc89b143"
      },
      "source": [
        "print(es.indices.get('document'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'document': {'aliases': {}, 'mappings': {'properties': {'text': {'type': 'text', 'similarity': 'my_similarity', 'analyzer': 'my_analyzer'}, 'text_origin': {'type': 'text', 'similarity': 'my_similarity', 'analyzer': 'my_analyzer'}, 'title': {'type': 'keyword', 'copy_to': ['title_kor2eng', 'title_eng2kor']}, 'title_eng2kor': {'type': 'text', 'analyzer': 'my_analyzer', 'search_analyzer': 'eng2kor_analyzer'}, 'title_kor2eng': {'type': 'text', 'analyzer': 'my_analyzer', 'search_analyzer': 'kor2eng_analyzer'}}}, 'settings': {'index': {'number_of_shards': '1', 'provided_name': 'document', 'similarity': {'my_similarity': {'type': 'BM25'}}, 'creation_date': '1622432056348', 'analysis': {'filter': {'my_shingle_f': {'type': 'shingle'}}, 'analyzer': {'eng2kor_analyzer': {'filter': ['trim', 'lowercase', 'javacafe_eng2kor'], 'type': 'custom', 'tokenizer': 'nori_tokenizer'}, 'kor2eng_analyzer': {'filter': ['trim', 'lowercase', 'javacafe_kor2eng'], 'type': 'custom', 'tokenizer': 'nori_tokenizer'}, 'my_analyzer': {'filter': ['lowercase', 'my_shingle_f', 'nori_readingform', 'cjk_bigram', 'decimal_digit', 'stemmer', 'trim'], 'decompound_mode': 'mixed', 'type': 'custom', 'stopwords': '_korean_', 'synonyms': '_korean_', 'tokenizer': 'nori_tokenizer'}}}, 'number_of_replicas': '1', 'uuid': 'Xiz8rWC4RfWrpXfdWtWMRQ', 'version': {'created': '7000099'}}}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13L2pYSKTMHw"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('/content/drive/MyDrive/한양대학교/부스트캠프 AI Tech/[P3] 기계독해/wikipedia_documents.json', 'r') as f:\n",
        "    wiki_data = pd.DataFrame(json.load(f)).transpose()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o_IbxbcTMBl"
      },
      "source": [
        "wiki_data = wiki_data.drop_duplicates(['text']) # 3876\n",
        "\n",
        "wiki_data = wiki_data.reset_index()\n",
        "\n",
        "del wiki_data['index']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTssuekWWv6N"
      },
      "source": [
        "import re\n",
        "\n",
        "wiki_data['text_origin'] = wiki_data['text']\n",
        "\n",
        "wiki_data['text_origin'] = wiki_data['text_origin'].apply(lambda x : ' '.join(re.sub(r'''[^ \\r\\nㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9ぁ-ゔァ-ヴー々〆〤一-龥~₩!@#$%^&*()“”‘’《》≪≫〈〉『』「」＜＞_+|{}:\"<>?`\\-=\\\\[\\];',.\\/·]''', ' ', str(x.lower().strip())).split()))\n",
        "\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\\\n\\\\n',' '))\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\n\\n',' '))\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\\\n',' '))\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\n',' '))\n",
        "\n",
        "wiki_data['text'] = wiki_data['text'].apply(lambda x : ' '.join(re.sub(r'''[^ \\r\\nㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9~₩!@#$%^&*()_+|{}:\"<>?`\\-=\\\\[\\];',.\\/]''', ' ', str(x.lower().strip())).split()))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlQoyPLBWwB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a6e01ce-4802-497f-b4bf-ed98e28feaca"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "title = []\n",
        "text = []\n",
        "text_origin = []\n",
        "\n",
        "for num in tqdm(range(len(wiki_data))):\n",
        "    cnt = 0\n",
        "    while cnt < len(wiki_data['text'][num]):\n",
        "        title.append(wiki_data['title'][num])\n",
        "        text.append(wiki_data['text'][num][cnt:cnt+1000])\n",
        "        text_origin.append(wiki_data['text_origin'][num])\n",
        "        cnt+=1000"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 56737/56737 [00:01<00:00, 28667.61it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU3fttbiWwJm"
      },
      "source": [
        "df = pd.DataFrame({'title':title,'text':text,'text_origin':text_origin})"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skdg5N_xWwNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca53dbc5-3181-40b9-aabd-40e78ada762f"
      },
      "source": [
        "from elasticsearch import Elasticsearch, helpers\n",
        "\n",
        "buffer = []\n",
        "rows = 0\n",
        "\n",
        "for num in tqdm(range(len(df))):\n",
        "    article = {\"_id\": num,\n",
        "               \"_index\": \"document\", \n",
        "               \"title\" : df['title'][num],\n",
        "               \"text\" : df['text'][num],\n",
        "               \"text_origin\" : df['text_origin'][num]}\n",
        "\n",
        "    buffer.append(article)\n",
        "\n",
        "    rows += 1\n",
        "\n",
        "    if rows % 3000 == 0:\n",
        "        helpers.bulk(es, buffer)\n",
        "        buffer = []\n",
        "\n",
        "        print(\"Inserted {} articles\".format(rows), end=\"\\r\")\n",
        "\n",
        "if buffer:\n",
        "    helpers.bulk(es, buffer)\n",
        "\n",
        "print(\"Total articles inserted: {}\".format(rows))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 3000/69555 [00:12<04:42, 235.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 3000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  9%|▊         | 6000/69555 [00:27<04:39, 227.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 6000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 9000/69555 [00:40<04:28, 225.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 9000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 12000/69555 [00:49<03:47, 253.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 12000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 15000/69555 [01:01<03:36, 252.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 15000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 26%|██▌       | 18000/69555 [01:09<03:06, 277.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 18000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 30%|███       | 21000/69555 [01:20<02:57, 274.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 21000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 35%|███▍      | 24000/69555 [01:29<02:35, 292.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 24000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 27000/69555 [01:40<02:29, 284.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 27000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 30000/69555 [01:49<02:11, 300.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 30000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 33000/69555 [02:00<02:05, 291.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 33000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 52%|█████▏    | 36000/69555 [02:11<01:59, 280.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 36000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 39000/69555 [02:20<01:44, 292.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 39000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 42000/69555 [02:32<01:37, 284.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 42000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 65%|██████▍   | 45000/69555 [02:40<01:20, 306.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 45000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 69%|██████▉   | 48000/69555 [02:50<01:11, 299.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 48000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 51000/69555 [02:58<00:57, 321.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 51000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 54000/69555 [03:11<00:53, 291.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 54000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 57000/69555 [03:18<00:39, 315.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 57000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 86%|████████▋ | 60000/69555 [03:29<00:31, 303.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 60000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 91%|█████████ | 63000/69555 [03:37<00:20, 325.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 63000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 95%|█████████▍| 66000/69555 [03:48<00:11, 304.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 66000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 69555/69555 [03:57<00:00, 293.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inserted 69000 articles\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total articles inserted: 69555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78ykg2T7TSs-"
      },
      "source": [
        "from typing import Optional, Dict, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from fairseq.models.roberta import RobertaHubInterface, RobertaModel\n",
        "\n",
        "from pororo.models.brainbert.utils import softmax\n",
        "from pororo.tasks.utils.download_utils import download_or_load\n",
        "from pororo.tasks.utils.tokenizer import CustomTokenizer\n",
        "from pororo.tasks.utils.base import PororoBiencoderBase, PororoFactoryBase"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Emx1aFwTSv7"
      },
      "source": [
        "class PororoMrcFactory(PororoFactoryBase):\n",
        "\n",
        "    def __init__(self, task: str, lang: str, model: Optional[str]):\n",
        "        super().__init__(task, lang, model)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_available_langs():\n",
        "        return [\"ko\"]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_available_models():\n",
        "        return {\"ko\": [\"brainbert.base.ko.korquad\"]}\n",
        "\n",
        "    def load(self, device: str):\n",
        "\n",
        "        if \"brainbert\" in self.config.n_model:\n",
        "            try:\n",
        "                import mecab\n",
        "            except ModuleNotFoundError as error:\n",
        "                raise error.__class__(\n",
        "                    \"Please install python-mecab-ko with: `pip install python-mecab-ko`\"\n",
        "                )\n",
        "\n",
        "            from pororo.utils import postprocess_span\n",
        "\n",
        "            model = (My_BrainRobertaModel.load_model(\n",
        "                f\"bert/{self.config.n_model}\",\n",
        "                self.config.lang,\n",
        "            ).eval().to(device))\n",
        "\n",
        "            tagger = mecab.MeCab()\n",
        "\n",
        "            return PororoBertMrc(model, tagger, postprocess_span, self.config)\n",
        "\n",
        "class My_BrainRobertaModel(RobertaModel):\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, model_name: str, lang: str, **kwargs):\n",
        "\n",
        "        from fairseq import hub_utils\n",
        "\n",
        "        ckpt_dir = download_or_load(model_name, lang)\n",
        "        tok_path = download_or_load(f\"tokenizers/bpe32k.{lang}.zip\", lang)\n",
        "\n",
        "        x = hub_utils.from_pretrained(\n",
        "            ckpt_dir,\n",
        "            \"model.pt\",\n",
        "            ckpt_dir,\n",
        "            load_checkpoint_heads=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "        return BrainRobertaHubInterface(\n",
        "            x[\"args\"],\n",
        "            x[\"task\"],\n",
        "            x[\"models\"][0],\n",
        "            tok_path,\n",
        "        )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PIoCwN3TS1a"
      },
      "source": [
        "class BrainRobertaHubInterface(RobertaHubInterface):\n",
        "\n",
        "    def __init__(self, args, task, model, tok_path):\n",
        "        super().__init__(args, task, model)\n",
        "        self.bpe = CustomTokenizer.from_file(\n",
        "            vocab_filename=f\"{tok_path}/vocab.json\",\n",
        "            merges_filename=f\"{tok_path}/merges.txt\",\n",
        "        )\n",
        "\n",
        "    def tokenize(self, sentence: str, add_special_tokens: bool = False):\n",
        "        result = \" \".join(self.bpe.encode(sentence).tokens)\n",
        "        if add_special_tokens:\n",
        "            result = f\"<s> {result} </s>\"\n",
        "        return result\n",
        "\n",
        "    def encode(\n",
        "        self,\n",
        "        sentence: str,\n",
        "        *addl_sentences,\n",
        "        add_special_tokens: bool = True,\n",
        "        no_separator: bool = False,\n",
        "    ) -> torch.LongTensor:\n",
        "\n",
        "        bpe_sentence = self.tokenize(\n",
        "            sentence,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "        )\n",
        "\n",
        "        for s in addl_sentences:\n",
        "            bpe_sentence += \" </s>\" if not no_separator and add_special_tokens else \"\"\n",
        "            bpe_sentence += (\" \" + self.tokenize(s, add_special_tokens=False) +\n",
        "                             \" </s>\" if add_special_tokens else \"\")\n",
        "        tokens = self.task.source_dictionary.encode_line(\n",
        "            bpe_sentence,\n",
        "            append_eos=False,\n",
        "            add_if_not_exist=False,\n",
        "        )\n",
        "        return tokens.long()\n",
        "\n",
        "    def decode(\n",
        "        self,\n",
        "        tokens: torch.LongTensor,\n",
        "        skip_special_tokens: bool = True,\n",
        "        remove_bpe: bool = True,\n",
        "    ) -> str:\n",
        "        assert tokens.dim() == 1\n",
        "        tokens = tokens.numpy()\n",
        "\n",
        "        if tokens[0] == self.task.source_dictionary.bos(\n",
        "        ) and skip_special_tokens:\n",
        "            tokens = tokens[1:]\n",
        "\n",
        "        eos_mask = tokens == self.task.source_dictionary.eos()\n",
        "        doc_mask = eos_mask[1:] & eos_mask[:-1]\n",
        "        sentences = np.split(tokens, doc_mask.nonzero()[0] + 1)\n",
        "\n",
        "        if skip_special_tokens:\n",
        "            sentences = [\n",
        "                np.array(\n",
        "                    [c\n",
        "                     for c in s\n",
        "                     if c != self.task.source_dictionary.eos()])\n",
        "                for s in sentences\n",
        "            ]\n",
        "\n",
        "        sentences = [\n",
        "            \" \".join([self.task.source_dictionary.symbols[c]\n",
        "                      for c in s])\n",
        "            for s in sentences\n",
        "        ]\n",
        "\n",
        "        if remove_bpe:\n",
        "            sentences = [\n",
        "                s.replace(\" \", \"\").replace(\"▁\", \" \").strip() for s in sentences\n",
        "            ]\n",
        "        if len(sentences) == 1:\n",
        "            return sentences[0]\n",
        "        return sentences\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_span(\n",
        "        self,\n",
        "        question: str,\n",
        "        context: str,\n",
        "        add_special_tokens: bool = True,\n",
        "        no_separator: bool = False,\n",
        "    ) -> Tuple:\n",
        "\n",
        "        max_length = self.task.max_positions()\n",
        "        tokens = self.encode(\n",
        "            question,\n",
        "            context,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            no_separator=no_separator,\n",
        "        )[:max_length]\n",
        "        with torch.no_grad():\n",
        "            logits = self.predict(\n",
        "                \"span_prediction_head\",\n",
        "                tokens,\n",
        "                return_logits=True,\n",
        "            ).squeeze()\n",
        "\n",
        "            results = []\n",
        "\n",
        "            top_n = 10\n",
        "            \n",
        "            starts = logits[:,0].argsort(descending = True)[:top_n].tolist()\n",
        "\n",
        "            for start in starts:\n",
        "                ends = logits[:,1].argsort(descending = True).tolist()\n",
        "                masked_ends = [end for end in ends if end >= start ]\n",
        "                ends = (masked_ends+ends)[:top_n]\n",
        "                for end in ends:\n",
        "                    answer_tokens = tokens[start:end + 1]\n",
        "                    answer = \"\"\n",
        "                    if len(answer_tokens) >= 1:\n",
        "                        decoded = self.decode(answer_tokens)\n",
        "                        if isinstance(decoded, str):\n",
        "                            answer = decoded\n",
        "\n",
        "                    score = ((logits[:,0][start] + 5) * (logits[:,1][end] + 5)).item()\n",
        "                    results.append((answer, (start, end + 1), score))\n",
        "\n",
        "            ends = logits[:,1].argsort(descending = True)[:top_n].tolist()\n",
        "\n",
        "            for end in ends:\n",
        "                starts = logits[:,0].argsort(descending = True).tolist()\n",
        "                masked_starts = [start for start in starts if start >= end ]\n",
        "                starts = (masked_starts+starts)[:top_n]\n",
        "                for start in starts:\n",
        "                    answer_tokens = tokens[start:end + 1]\n",
        "                    answer = \"\"\n",
        "                    if len(answer_tokens) >= 1:\n",
        "                        decoded = self.decode(answer_tokens)\n",
        "                        if isinstance(decoded, str):\n",
        "                            answer = decoded\n",
        "\n",
        "                    score = ((logits[:,0][start] + 5) * (logits[:,1][end] + 5)).item()\n",
        "                    results.append((answer, (start, end + 1), score))\n",
        "            \n",
        "        return results"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozjFxW1eTS6w"
      },
      "source": [
        "class PororoBertMrc(PororoBiencoderBase):\n",
        "\n",
        "    def __init__(self, model, tagger, callback, config):\n",
        "        super().__init__(config)\n",
        "        self._model = model\n",
        "        self._tagger = tagger\n",
        "        self._callback = callback\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        query: str,\n",
        "        context: str,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[str, Tuple[int, int]]:\n",
        "\n",
        "        postprocess = kwargs.get(\"postprocess\", True)\n",
        "\n",
        "        pair_results = self._model.predict_span(query, context)\n",
        "        returns = []\n",
        "        \n",
        "        for pair_result in pair_results:\n",
        "            span = self._callback(\n",
        "            self._tagger,\n",
        "            pair_result[0],\n",
        "            ) if postprocess else pair_result[0]\n",
        "            returns.append((span,pair_result[1],pair_result[2]))\n",
        "        \n",
        "        return returns"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7dZWruFTTCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea310ee8-8378-4e85-90d3-91ac5cec2237"
      },
      "source": [
        "from pororo import Pororo\n",
        "ner = Pororo(task=\"ner\", lang=\"ko\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awaNWQNqSHZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ccdfca6-0dfd-410c-f4c0-4bd7556485f1"
      },
      "source": [
        "mrc_factory = PororoMrcFactory('mrc', 'ko', \"brainbert.base.ko.korquad\")\n",
        "mrc = mrc_factory.load(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zUQkWxokHZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7d7047-8cb2-4fe5-82e4-9a674e2f9439"
      },
      "source": [
        "from flask import Flask, render_template, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__,static_folder='/content/ODQA-Demo-Site/static',template_folder='/content/ODQA-Demo-Site/templates')\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/get\")\n",
        "def get_bot_response():\n",
        "    question = request.args.get('msg')\n",
        "    query = {\n",
        "        'query':{\n",
        "            'bool':{\n",
        "                'must':[\n",
        "                        {'match':{'text':question}}\n",
        "                ],\n",
        "                'should':[\n",
        "                        {'match':{'text':' '.join([i[0] for i in ner(question) if i[1] != 'O'])}}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    doc = es.search(index='document',body=query,size=10)['hits']['hits']\n",
        "    ans_lst = []\n",
        "    max_scr = doc[0]['_score']\n",
        "    for i in range(len(doc)):\n",
        "        ans = mrc(question,doc[0]['_source']['text'],postprocess=False)[0]\n",
        "        if ans == '' or 'unk' in ans or len(ans) >= 30:\n",
        "            pass\n",
        "        else:\n",
        "            ans_lst.append((ans,ans[1],ans[2]*doc[i]['_score']/max_scr))\n",
        "    res = sorted(ans_lst, key = lambda x : x[2], reverse=True)[0][0][0]\n",
        "    return res\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://b2472c73e0a7.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}